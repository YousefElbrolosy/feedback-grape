{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81979c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import os\n",
    "\n",
    "os.sys.path.append(\"..\")\n",
    "from feedback_grape.fgrape import optimize_pulse_with_feedback\n",
    "from feedback_grape.utils.operators import (\n",
    "    sigmap,\n",
    "    sigmam,\n",
    "    create,\n",
    "    destroy,\n",
    "    identity,\n",
    "    cosm,\n",
    "    sinm,\n",
    ")\n",
    "from feedback_grape.utils.states import basis, fock\n",
    "from feedback_grape.utils.tensor import tensor\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import expm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4baf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cav = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c912a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qubit_unitary(alpha):\n",
    "    \"\"\"\n",
    "    TODO: see if alpha, can be sth elser other than scalar, and if the algo understands this\n",
    "    see if there can be multiple params like alpha and beta input\n",
    "    \"\"\"\n",
    "    return expm(\n",
    "        -1j\n",
    "        * (\n",
    "            alpha * tensor(identity(N_cav), sigmap())\n",
    "            + alpha.conjugate() * tensor(identity(N_cav), sigmam())\n",
    "        )\n",
    "        / 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1baa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qubit_cavity_unitary(beta):\n",
    "    return expm(\n",
    "        -1j\n",
    "        * (\n",
    "            beta\n",
    "            * (\n",
    "                tensor(destroy(N_cav), identity(2))\n",
    "                @ tensor(identity(N_cav), sigmap())\n",
    "            )\n",
    "            + beta.conjugate()\n",
    "            * (\n",
    "                tensor(create(N_cav), identity(2))\n",
    "                @ tensor(identity(N_cav), sigmam())\n",
    "            )\n",
    "        )\n",
    "        / 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a6a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.operators import create, destroy\n",
    "\n",
    "\n",
    "def povm_measure_operator(measurement_outcome, gamma, delta):\n",
    "    \"\"\"\n",
    "    POVM for the measurement of the cavity state.\n",
    "    returns Mm ( NOT the POVM element Em = Mm_dag @ Mm ), given measurement_outcome m, gamma and delta\n",
    "    \"\"\"\n",
    "    number_operator = tensor(create(N_cav) @ destroy(N_cav), identity(2))\n",
    "    angle = (gamma * number_operator) + delta / 2\n",
    "    meas_op = jnp.where(\n",
    "        measurement_outcome == 1,\n",
    "        cosm(angle),\n",
    "        sinm(angle),\n",
    "    )\n",
    "    return meas_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e88143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial state is a thermal state coupled to a qubit in the ground state?\n",
    "n_average = 9\n",
    "# natural logarithm\n",
    "beta = jnp.log((1 / n_average) + 1)\n",
    "diags = jnp.exp(-beta * jnp.arange(N_cav))\n",
    "normalized_diags = diags / jnp.sum(diags, axis=0)\n",
    "rho_cav = jnp.diag(normalized_diags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb3eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho0 = tensor(rho_cav, basis(2, 0) @ basis(2, 0).conj().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47908f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 60)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce889fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.states import coherent\n",
    "alpha = 3\n",
    "psi_target = tensor(coherent(N_cav, alpha) + coherent(N_cav, -alpha) + coherent(N_cav, 1j*alpha) + coherent(N_cav, -1j*alpha), basis(2)) # 4-legged state\n",
    "\n",
    "rho_target = psi_target @ psi_target.conj().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2321d333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6439b369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20605832342551042\n"
     ]
    }
   ],
   "source": [
    "from feedback_grape.utils.fidelity import fidelity\n",
    "\n",
    "print(fidelity(U_final=rho0, C_target=rho_target, type=\"density\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab29059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "class RNN(nn.Module):\n",
    "    hidden_size: int  # number of features in the hidden state\n",
    "    output_size: int  # number of features in the output ( 2 in the case of gamma and beta)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, measurement, hidden_state):\n",
    "        \"\"\"\n",
    "        If your GRU has a hidden state increasing number of features in the hidden stateH means:\n",
    "\n",
    "        - You're allowing the model to store more information across time steps\n",
    "\n",
    "        - Each time step can represent more complex features, patterns, or dependencies\n",
    "\n",
    "        - You're giving the GRU more representational capacity\n",
    "        \"\"\"\n",
    "        gru_cell = nn.GRUCell(features=self.hidden_size)\n",
    "        self.make_rng('dropout')\n",
    "        if measurement.ndim == 1:\n",
    "            measurement = measurement.reshape(1, -1)\n",
    "        new_hidden_state, _ = gru_cell(hidden_state, measurement)\n",
    "        new_hidden_state = nn.Dropout(rate=0.2, deterministic=False)(\n",
    "            new_hidden_state\n",
    "        )\n",
    "        # this returns the povm_params after linear regression through the hidden state which contains\n",
    "        # the information of the previous time steps and this is optimized to output best povm_params\n",
    "        # new_hidden_state = nn.Dense(features=self.hidden_size)(new_hidden_state)\n",
    "        output = nn.Dense(\n",
    "            features=self.output_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "            bias_init=nn.initializers.constant(jnp.pi),\n",
    "        )(new_hidden_state)\n",
    "        # output = jnp.asarray(output)\n",
    "        return output[0], new_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5329d87d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got int64. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# avg_photon_numer = 2 When testing kitten state\u001b[39;00m\n\u001b[32m      7\u001b[39m initial_params = {\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPOVM\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m],\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mU_q\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m0\u001b[39m],\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mU_qc\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m0\u001b[39m],\n\u001b[32m     11\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m result = \u001b[43moptimize_pulse_with_feedback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mU_0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrho0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mC_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrho_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameterized_gates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpovm_measure_operator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqubit_unitary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqubit_cavity_unitary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmeasurement_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# decay= {\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     \"decay_indices\": [0,1], # indices of gates before which decay occurs\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     \"decay_rates\": [0.01, 0.01], # decay rates for each gate\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     \"decay_durations\": [1, 1], # duration of decay for each gate\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# }\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_time_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_time_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlookup\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoal\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfidelity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madam\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_of_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdensity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# RNN=RNN\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/FeedBack-GRAPE/feedback_paper_examples/../feedback_grape/fgrape.py:506\u001b[39m, in \u001b[36moptimize_pulse_with_feedback\u001b[39m\u001b[34m(U_0, C_target, parameterized_gates, measurement_indices, initial_params, goal, mode, num_time_steps, optimizer, max_iter, convergence_threshold, learning_rate, type, batch_size, decay, RNN)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss1 + loss2\n\u001b[32m    504\u001b[39m key, sub_key = jax.random.split(parent_rng_key)\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m best_model_params, iter_idx = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m result = evaluate(\n\u001b[32m    517\u001b[39m     U_0=U_0,\n\u001b[32m    518\u001b[39m     C_target=C_target,\n\u001b[32m   (...)\u001b[39m\u001b[32m    532\u001b[39m     num_iterations=iter_idx,\n\u001b[32m    533\u001b[39m )\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/FeedBack-GRAPE/feedback_paper_examples/../feedback_grape/fgrape.py:553\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(optimizer, loss_fn, trainable_params, prng_key, max_iter, learning_rate, convergence_threshold)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# Optimization\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# set up optimizer and training state\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer.upper() == \u001b[33m\"\u001b[39m\u001b[33mADAM\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     best_model_params, iter_idx = \u001b[43m_optimize_adam_feedback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m optimizer.upper() == \u001b[33m\"\u001b[39m\u001b[33mL-BFGS\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    562\u001b[39m     \u001b[38;5;66;03m# TODO: implement L-BFGS for feedback version\u001b[39;00m\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    564\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mL-BFGS optimizer is not implemented for feedback version yet.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    565\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/FeedBack-GRAPE/feedback_paper_examples/../feedback_grape/utils/optimizers.py:68\u001b[39m, in \u001b[36m_optimize_adam_feedback\u001b[39m\u001b[34m(loss_fn, control_amplitudes, max_iter, learning_rate, convergence_threshold, key)\u001b[39m\n\u001b[32m     66\u001b[39m iter_idx = -\u001b[32m1\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iter_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     params, opt_state, loss, key = \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     losses.append(loss)\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m     71\u001b[39m         iter_idx > \u001b[32m0\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(losses[-\u001b[32m1\u001b[39m] - losses[-\u001b[32m2\u001b[39m]) < convergence_threshold\n\u001b[32m     73\u001b[39m     ):\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/FeedBack-GRAPE/feedback_paper_examples/../feedback_grape/utils/optimizers.py:58\u001b[39m, in \u001b[36m_optimize_adam_feedback.<locals>.step\u001b[39m\u001b[34m(params, state, key)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;129m@jax\u001b[39m.jit\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(params, state, key):\n\u001b[32m     57\u001b[39m     loss = loss_fn(params, key)  \u001b[38;5;66;03m# Minimize -loss_fn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     grads = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     updates, new_state = optimizer.update(grads, state, params)\n\u001b[32m     60\u001b[39m     new_params = optax.apply_updates(params, updates)\n",
      "    \u001b[31m[... skipping hidden 4 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/FeedBack-GRAPE/.conda/lib/python3.11/site-packages/jax/_src/api.py:511\u001b[39m, in \u001b[36m_check_input_dtype_revderiv\u001b[39m\u001b[34m(name, holomorphic, allow_int, x)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (dtypes.issubdtype(aval.dtype, dtypes.extended) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    508\u001b[39m     dtypes.issubdtype(aval.dtype, np.integer) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    509\u001b[39m     dtypes.issubdtype(aval.dtype, np.bool_)):\n\u001b[32m    510\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_int:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires real- or complex-valued inputs (input dtype \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    512\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthat is a sub-dtype of np.inexact), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval.dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mIf you want to use Boolean- or integer-valued inputs, use vjp \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mor set allow_int to True.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtypes.issubdtype(aval.dtype, np.inexact):\n\u001b[32m    516\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires numerical-valued inputs (input dtype that is a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    517\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msub-dtype of np.bool_ or np.number), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval.dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got int64. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True."
     ]
    }
   ],
   "source": [
    "num_time_steps = 10\n",
    "num_of_iterations = 10\n",
    "learning_rate = 0.05\n",
    "# avg_photon_numer = 2 When testing kitten state\n",
    "\n",
    "\n",
    "initial_params = {\n",
    "    \"POVM\": [0.0, 0.0],\n",
    "    \"U_q\": [0.0],\n",
    "    \"U_qc\": [0.0],\n",
    "}\n",
    "\n",
    "\n",
    "result = optimize_pulse_with_feedback(\n",
    "    U_0=rho0,\n",
    "    C_target=rho_target,\n",
    "    parameterized_gates=[\n",
    "        povm_measure_operator,\n",
    "        qubit_unitary,\n",
    "        qubit_cavity_unitary,\n",
    "    ],\n",
    "    measurement_indices=[0],\n",
    "    # decay= {\n",
    "    #     \"decay_indices\": [0,1], # indices of gates before which decay occurs\n",
    "    #     \"decay_rates\": [0.01, 0.01], # decay rates for each gate\n",
    "    #     \"decay_durations\": [1, 1], # duration of decay for each gate\n",
    "    # }\n",
    "    initial_params=initial_params,\n",
    "    num_time_steps=num_time_steps,\n",
    "    mode=\"lookup\",\n",
    "    goal=\"fidelity\",\n",
    "    optimizer=\"adam\",\n",
    "    max_iter=num_of_iterations,\n",
    "    convergence_threshold=1e-20,\n",
    "    learning_rate=learning_rate,\n",
    "    type=\"density\",\n",
    "    batch_size=10,\n",
    "    # RNN=RNN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242b562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2243634091867723\n"
     ]
    }
   ],
   "source": [
    "print(result.final_fidelity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546d79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial fidelity: 0.20605832342551042\n",
      "fidelity of state 0: 0.2202416953439082\n",
      "fidelity of state 1: 0.2122541896298968\n",
      "fidelity of state 2: 0.24418109580390757\n",
      "fidelity of state 3: 0.14616579116529232\n",
      "fidelity of state 4: 0.2989742739908566\n"
     ]
    }
   ],
   "source": [
    "from feedback_grape.utils.fidelity import fidelity\n",
    "\n",
    "print(\n",
    "    \"initial fidelity:\",\n",
    "    fidelity(C_target=rho_target, U_final=rho0, type=\"density\"),\n",
    ")\n",
    "for i, state in enumerate(result.final_state):\n",
    "    print(\n",
    "        f\"fidelity of state {i}:\",\n",
    "        fidelity(C_target=rho_target, U_final=state, type=\"density\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5fd59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
