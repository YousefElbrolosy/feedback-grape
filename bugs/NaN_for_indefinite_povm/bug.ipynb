{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26661a6",
   "metadata": {},
   "source": [
    "ISSUE: In this code (which is based upon example_I), NaN values are produced in the fgrape update gradients in optimization step 234. Interestingly, this was only observed for LUT method in example_I.\n",
    "\n",
    "CAUSE: Apparently, the optimizer has problems if parametrized eigenvalues of POVM elements become zero. I interprete that this has to do with probabilities becoming zeros which could cause zero-division errors. In the system studied here, the issue was solved by parametrizing POVM elements such that their eigenvalues are bound between 1e-6 and 1 - 1e-6 instead of bounds between 0 and 1 in line 145 of bug_helpers.py.\n",
    "\n",
    "Other unsuccessfull attemps of solving the issue were:\n",
    "- Trying out different methods in dynamiqs.mesolve for solving lindblad equation. Only Kvaerno3 was successfull in the example tried out, at the cost of 300x more runtime. There is no guarantee that it always works.\n",
    "- Trying out different error tolerances (atol=1e-10, rtol=1e-10) in mesolve Tsit5 method.\n",
    "- Trying out different values for eps and eps_root in frgrape's adam optimizer, which should avoid division by zeros. It solved some examples but caused the same issue in others.\n",
    "- Changing the clipping of prob in .utils.povm from \"jnp.maximum(prob, 1e-10)\" to \"prob + 1e-6\".\n",
    "- Skipping of updates where nan is detected and re-evaluating batch with different keys -> the re-evaluated batches mostly produced nan values aswell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c7e75",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feedback_grape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m sys.path.append(os.path.abspath(\u001b[33m\"\u001b[39m\u001b[33m./../\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ruff: noqa\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfeedback_grape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfgrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize_pulse \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbugs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNaN_for_indefinite_povm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbug_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     init_fgrape_protocol,\n\u001b[32m      9\u001b[39m     test_implementations,\n\u001b[32m     10\u001b[39m     generate_random_state,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'feedback_grape'"
     ]
    }
   ],
   "source": [
    "# ruff: noqa\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"./../../\"))\n",
    "\n",
    "# ruff: noqa\n",
    "from feedback_grape.fgrape import optimize_pulse # type: ignore\n",
    "from bugs.NaN_for_indefinite_povm.bug_helpers import (\n",
    "    init_fgrape_protocol,\n",
    "    test_implementations,\n",
    "    generate_random_state,\n",
    ")\n",
    "\n",
    "import jax\n",
    "\n",
    "test_implementations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical parameters\n",
    "# (attention! #elements in density matrix grow as 4^n*N_chains)\n",
    "n = 2 # number of qubits per chain (>= 3)\n",
    "N_chains = 2 # Number of parallel chains to simulate\n",
    "gamma = 0.25 # Decay constant\n",
    "\n",
    "# Training and evaluation parameters\n",
    "training_params = {\n",
    "    \"N_training_iterations\": 1000, # Number of training iterations\n",
    "    \"learning_rate\": 0.02, # Learning rate\n",
    "    \"convergence_threshold\": 1e-6,\n",
    "    \"batch_size\": 16,\n",
    "    \"eval_batch_size\": 16,\n",
    "}\n",
    "\n",
    "# Parameters to test\n",
    "\n",
    "#num_time_steps : Number of time steps in the control pulse\n",
    "#lut_depth : Depth of the lookup table for feedback\n",
    "#reward_weights: Weights for the reward at each time step. Default only weights last timestep [0, 0, ... 0, 1]\n",
    "\n",
    "num_time_steps, lut_depth, reward_weights = 2, 1, [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac0ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Loss: -0.455816, T=0s, eta=47s\n",
      "Iteration 20, Loss: -0.638927, T=0s, eta=46s\n",
      "Iteration 30, Loss: -0.660276, T=1s, eta=46s\n",
      "Iteration 40, Loss: -1.107681, T=1s, eta=45s\n",
      "Iteration 50, Loss: -1.489177, T=2s, eta=45s\n",
      "Iteration 60, Loss: -1.324170, T=2s, eta=44s\n",
      "Iteration 70, Loss: -1.447311, T=3s, eta=44s\n",
      "Iteration 80, Loss: -1.474644, T=3s, eta=43s\n",
      "Iteration 90, Loss: -1.191468, T=4s, eta=43s\n",
      "Iteration 100, Loss: -1.486097, T=4s, eta=43s\n",
      "Iteration 110, Loss: -0.753588, T=5s, eta=42s\n",
      "Iteration 120, Loss: -0.662744, T=5s, eta=42s\n",
      "Iteration 130, Loss: -1.456799, T=6s, eta=41s\n",
      "Iteration 140, Loss: -1.307937, T=6s, eta=41s\n",
      "Iteration 150, Loss: -1.363153, T=7s, eta=40s\n",
      "Iteration 160, Loss: -1.199992, T=7s, eta=40s\n",
      "Iteration 170, Loss: -1.225331, T=8s, eta=39s\n",
      "Iteration 180, Loss: -1.737238, T=8s, eta=39s\n",
      "Iteration 190, Loss: -1.547859, T=9s, eta=38s\n",
      "Iteration 200, Loss: -0.903251, T=9s, eta=38s\n",
      "Iteration 210, Loss: -0.983188, T=10s, eta=37s\n",
      "Iteration 220, Loss: -1.725295, T=10s, eta=37s\n",
      "Iteration 230, Loss: -0.764716, T=11s, eta=36s\n",
      "Warning: NaN values detected in updated parameters at iteration 234. Stopping optimization.\n",
      "Info: NaN values may occur due to high learning rates or POVM elements with zero eigenvalues.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state_callable = lambda key: generate_random_state(key, N_chains=N_chains)\n",
    "\n",
    "system_params = init_fgrape_protocol(jax.random.PRNGKey(0), n, N_chains, gamma)\n",
    "\n",
    "result = optimize_pulse(\n",
    "    U_0=state_callable,\n",
    "    C_target=state_callable,\n",
    "    system_params=system_params,\n",
    "    num_time_steps=num_time_steps,\n",
    "    lut_depth=lut_depth,\n",
    "    reward_weights=reward_weights,\n",
    "    mode=\"lookup\",\n",
    "    goal=\"fidelity\",\n",
    "    max_iter=training_params[\"N_training_iterations\"],\n",
    "    convergence_threshold=training_params[\"convergence_threshold\"],\n",
    "    learning_rate=training_params[\"learning_rate\"],\n",
    "    evo_type=\"density\",\n",
    "    batch_size=training_params[\"batch_size\"],\n",
    "    eval_batch_size=training_params[\"eval_batch_size\"],\n",
    "    progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688bbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
