{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76d2ebd",
   "metadata": {},
   "source": [
    "# D. State stabilization in a noisy environment with Jaynes-Cummings controls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018ebc9",
   "metadata": {},
   "source": [
    "Here the result is as follows: The algorithm optimizes the params, such that the POVM always outputs 1, implying that the measurement leaves the target state invariant. this is what we are indeed seeing when printing the measurement outcome and its probability, when batching however, the optimizer struggles to converge.\n",
    "\n",
    "Also, lookup here is much better than nn for the same hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d5142",
   "metadata": {},
   "source": [
    "This is actually a perfect example, of feedback grape modifying the params so that a certain measurement sequence will always be output because this measurement sequence is the one that is going to lead to the best fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "408d7729",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import os\n",
    "\n",
    "os.sys.path.append(\"../../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d81979c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.fgrape import optimize_pulse\n",
    "from feedback_grape.utils.operators import (\n",
    "    sigmap,\n",
    "    sigmam,\n",
    "    create,\n",
    "    destroy,\n",
    "    identity,\n",
    "    cosm,\n",
    "    sinm,\n",
    ")\n",
    "from feedback_grape.utils.states import basis, fock\n",
    "from feedback_grape.utils.tensor import tensor\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import expm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fe4baf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cav = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba9d984",
   "metadata": {},
   "source": [
    "## Here, dividing alpha into real and imaginary parts complicates the optimization and converges at 0.89 while if we do not use the imaginary part it converges at 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1c912a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qubit_unitary(alpha_re):\n",
    "    alpha = alpha_re[0]\n",
    "    return tensor(\n",
    "        identity(N_cav),\n",
    "        expm(-1j * (alpha * sigmap() + alpha.conjugate() * sigmam()) / 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c1baa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qubit_cavity_unitary(beta_re):\n",
    "    beta = beta_re[0]\n",
    "    return expm(\n",
    "        -1j\n",
    "        * (\n",
    "            beta * (tensor(destroy(N_cav), sigmap()))\n",
    "            + beta.conjugate() * (tensor(create(N_cav), sigmam()))\n",
    "        )\n",
    "        / 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18095d1e",
   "metadata": {},
   "source": [
    "### povm_measure_operator (callable): <br>\n",
    "    - It should take a measurement outcome and list of params as input\n",
    "    - The measurement outcome options are either 1 or -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "70a6a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.operators import create, destroy\n",
    "\n",
    "\n",
    "def povm_measure_operator(measurement_outcome, params):\n",
    "    \"\"\"\n",
    "    POVM for the measurement of the cavity state.\n",
    "    returns Mm ( NOT the POVM element Em = Mm_dag @ Mm ), given measurement_outcome m, gamma and delta\n",
    "    \"\"\"\n",
    "    gamma, delta = params\n",
    "    number_operator = tensor(create(N_cav) @ destroy(N_cav), identity(2))\n",
    "    angle = (gamma * number_operator) + delta / 2 * identity(2*N_cav)\n",
    "    meas_op = jnp.where(\n",
    "        measurement_outcome == 1,\n",
    "        cosm(angle),\n",
    "        sinm(angle),\n",
    "    )\n",
    "    return meas_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ce889fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.states import coherent\n",
    "\n",
    "alpha = 3\n",
    "psi_target = tensor(\n",
    "    coherent(N_cav, alpha)\n",
    "    + coherent(N_cav, -alpha)\n",
    "    + coherent(N_cav, 1j * alpha)\n",
    "    + coherent(N_cav, -1j * alpha),\n",
    "    basis(2),\n",
    ")  # 4-legged state\n",
    "\n",
    "# Normalize psi_target before constructing rho_target\n",
    "psi_target = psi_target / jnp.linalg.norm(psi_target)\n",
    "rho_target = psi_target @ psi_target.conj().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2321d333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 60)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bfc6e",
   "metadata": {},
   "source": [
    "### It is important to test what the POVM probability is, to check if your state is normalized. if the probability is bounded between 0 and 1 then normalized\n",
    "This completeness condition on the measurement operators is automatically checked for the `initial_params` in `optimize_pulse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6599acde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.09369074, dtype=float64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer: should one normalize within the optimization just in case?\n",
    "# --> no so that user is not misled into thinking that his unnormalized state\n",
    "# is working properly\n",
    "from feedback_grape.utils.povm import (\n",
    "    _probability_of_a_measurement_outcome_given_a_certain_state,\n",
    ")\n",
    "\n",
    "_probability_of_a_measurement_outcome_given_a_certain_state(\n",
    "    rho_target,\n",
    "    +1,\n",
    "    povm_measure_operator(+1, params=[0.058, jnp.pi / 2]),\n",
    "    povm_measure_operator(-1, params=[0.058, jnp.pi / 2]),\n",
    "    evo_type=\"density\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6439b369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000093148405\n"
     ]
    }
   ],
   "source": [
    "from feedback_grape.utils.fidelity import fidelity\n",
    "\n",
    "print(fidelity(U_final=rho_target, C_target=rho_target, evo_type=\"density\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f2b30f",
   "metadata": {},
   "source": [
    "### Without dissipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5329d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Loss: -0.434777, T=0s, eta=51s\n",
      "Iteration 20, Loss: -0.469147, T=1s, eta=55s\n",
      "Iteration 30, Loss: -0.430738, T=1s, eta=53s\n",
      "Iteration 40, Loss: -0.414021, T=2s, eta=51s\n",
      "Iteration 50, Loss: -0.425890, T=2s, eta=51s\n",
      "Iteration 60, Loss: -0.405357, T=3s, eta=50s\n",
      "Iteration 70, Loss: -0.408244, T=3s, eta=50s\n",
      "Iteration 80, Loss: -0.441015, T=4s, eta=51s\n",
      "Iteration 90, Loss: -0.460492, T=5s, eta=51s\n",
      "Iteration 100, Loss: 0.017883, T=5s, eta=50s\n",
      "Iteration 110, Loss: -0.471190, T=6s, eta=49s\n",
      "Iteration 120, Loss: -0.475588, T=6s, eta=48s\n",
      "Iteration 130, Loss: -0.479252, T=7s, eta=47s\n",
      "Iteration 140, Loss: -0.482890, T=7s, eta=46s\n",
      "Iteration 150, Loss: -0.482492, T=8s, eta=45s\n",
      "Iteration 160, Loss: -0.487008, T=8s, eta=44s\n",
      "Iteration 170, Loss: -0.491061, T=9s, eta=43s\n",
      "Iteration 180, Loss: -0.465822, T=9s, eta=43s\n",
      "Iteration 190, Loss: -0.312555, T=10s, eta=42s\n",
      "Iteration 200, Loss: -0.432598, T=10s, eta=42s\n",
      "Iteration 210, Loss: -0.496051, T=11s, eta=41s\n",
      "Iteration 220, Loss: -0.493484, T=11s, eta=41s\n",
      "Iteration 230, Loss: -0.496848, T=12s, eta=40s\n",
      "Iteration 240, Loss: -0.500379, T=12s, eta=39s\n",
      "Iteration 250, Loss: -0.501033, T=13s, eta=39s\n",
      "Iteration 260, Loss: -0.501868, T=13s, eta=38s\n",
      "Iteration 270, Loss: -0.502647, T=14s, eta=38s\n",
      "Iteration 280, Loss: -0.503082, T=14s, eta=37s\n",
      "Iteration 290, Loss: -0.499595, T=15s, eta=36s\n",
      "Iteration 300, Loss: -0.503120, T=15s, eta=36s\n",
      "Iteration 310, Loss: -0.504443, T=16s, eta=35s\n",
      "Iteration 320, Loss: -0.504665, T=16s, eta=35s\n",
      "Iteration 330, Loss: -0.505108, T=17s, eta=34s\n",
      "Iteration 340, Loss: 0.152348, T=17s, eta=34s\n",
      "Iteration 350, Loss: -0.497644, T=18s, eta=33s\n",
      "Iteration 360, Loss: -0.503711, T=18s, eta=32s\n",
      "Iteration 370, Loss: -0.505430, T=18s, eta=32s\n",
      "Iteration 380, Loss: -0.503113, T=19s, eta=31s\n",
      "Iteration 390, Loss: -0.489166, T=19s, eta=31s\n",
      "Iteration 400, Loss: -0.462798, T=20s, eta=30s\n",
      "Iteration 410, Loss: -0.497939, T=20s, eta=30s\n",
      "Iteration 420, Loss: -0.505822, T=21s, eta=29s\n",
      "Iteration 430, Loss: -0.504767, T=21s, eta=28s\n",
      "Iteration 440, Loss: 0.500319, T=22s, eta=28s\n",
      "Iteration 450, Loss: 0.202805, T=22s, eta=27s\n",
      "Iteration 460, Loss: -0.473746, T=23s, eta=27s\n",
      "Iteration 470, Loss: 0.504800, T=23s, eta=26s\n",
      "Iteration 480, Loss: -0.410197, T=24s, eta=26s\n",
      "Iteration 490, Loss: -0.468853, T=24s, eta=25s\n",
      "Iteration 500, Loss: -0.481122, T=25s, eta=25s\n",
      "Iteration 510, Loss: -0.480520, T=25s, eta=24s\n",
      "Iteration 520, Loss: -0.467256, T=26s, eta=24s\n",
      "Iteration 530, Loss: -0.421069, T=26s, eta=23s\n",
      "Iteration 540, Loss: -0.391984, T=27s, eta=23s\n",
      "Iteration 550, Loss: -0.474830, T=27s, eta=22s\n",
      "Iteration 560, Loss: -0.498058, T=28s, eta=22s\n",
      "Iteration 570, Loss: -0.466564, T=28s, eta=21s\n",
      "Iteration 580, Loss: -0.488221, T=29s, eta=21s\n",
      "Iteration 590, Loss: -0.506803, T=29s, eta=20s\n",
      "Iteration 600, Loss: 0.745416, T=29s, eta=19s\n",
      "Iteration 610, Loss: -0.507017, T=30s, eta=19s\n",
      "Iteration 620, Loss: -0.507870, T=30s, eta=18s\n",
      "Iteration 630, Loss: -0.508004, T=31s, eta=18s\n",
      "Iteration 640, Loss: 0.697659, T=31s, eta=17s\n",
      "Iteration 650, Loss: -0.508238, T=32s, eta=17s\n",
      "Iteration 660, Loss: -0.508146, T=32s, eta=16s\n",
      "Iteration 670, Loss: -0.494276, T=33s, eta=16s\n",
      "Iteration 680, Loss: -0.420752, T=33s, eta=15s\n",
      "Iteration 690, Loss: -0.353287, T=34s, eta=15s\n",
      "Iteration 700, Loss: 0.199136, T=34s, eta=14s\n",
      "Iteration 710, Loss: -0.351324, T=35s, eta=14s\n",
      "Iteration 720, Loss: -0.458880, T=35s, eta=13s\n",
      "Iteration 730, Loss: 0.507341, T=36s, eta=13s\n",
      "Iteration 740, Loss: -0.505615, T=36s, eta=12s\n",
      "Iteration 750, Loss: -0.506762, T=37s, eta=12s\n",
      "Iteration 760, Loss: 0.509360, T=37s, eta=11s\n",
      "Iteration 770, Loss: -0.506573, T=38s, eta=11s\n",
      "Iteration 780, Loss: -0.495912, T=38s, eta=10s\n",
      "Iteration 790, Loss: -0.461605, T=39s, eta=10s\n",
      "Iteration 800, Loss: -0.463453, T=39s, eta=9s\n",
      "Iteration 810, Loss: -0.502228, T=40s, eta=9s\n",
      "Iteration 820, Loss: -0.509080, T=40s, eta=8s\n",
      "Iteration 830, Loss: -0.509075, T=41s, eta=8s\n",
      "Iteration 840, Loss: -0.509425, T=41s, eta=7s\n",
      "Iteration 850, Loss: -0.508922, T=41s, eta=7s\n",
      "Iteration 860, Loss: -0.505178, T=42s, eta=6s\n",
      "Iteration 870, Loss: -0.508269, T=42s, eta=6s\n",
      "Iteration 880, Loss: 0.535438, T=43s, eta=5s\n",
      "Iteration 890, Loss: -0.509820, T=43s, eta=5s\n",
      "Iteration 900, Loss: 0.521650, T=44s, eta=4s\n",
      "Iteration 910, Loss: -0.510068, T=44s, eta=4s\n",
      "Iteration 920, Loss: -0.510148, T=45s, eta=3s\n",
      "Iteration 930, Loss: -0.510234, T=45s, eta=3s\n",
      "Iteration 940, Loss: -0.510330, T=46s, eta=2s\n",
      "Iteration 950, Loss: 0.524564, T=46s, eta=2s\n",
      "Iteration 960, Loss: 0.524688, T=47s, eta=2s\n",
      "Iteration 970, Loss: -0.509705, T=47s, eta=1s\n",
      "Iteration 980, Loss: -0.510633, T=48s, eta=1s\n",
      "Iteration 990, Loss: -0.510644, T=48s, eta=0s\n",
      "reward weights: (1, 1)\n",
      " final_fidelity: 0.0167309322812698\n",
      " final_purity: None\n",
      " purity_each_timestep: [1.00000001e+00 2.72884116e-04 6.35716027e-01 8.52075448e-03\n",
      " 3.26185438e-01 1.67309323e-02]\n",
      "\n",
      "Iteration 10, Loss: -0.435334, T=0s, eta=44s\n",
      "Iteration 20, Loss: -0.468315, T=0s, eta=45s\n",
      "Iteration 30, Loss: -0.455159, T=1s, eta=45s\n",
      "Iteration 40, Loss: -0.477642, T=1s, eta=45s\n",
      "Iteration 50, Loss: -0.463505, T=2s, eta=46s\n",
      "Iteration 60, Loss: 0.461153, T=2s, eta=46s\n",
      "Iteration 70, Loss: -0.414693, T=3s, eta=45s\n",
      "Iteration 80, Loss: -0.541859, T=3s, eta=45s\n",
      "Iteration 90, Loss: -0.602653, T=4s, eta=44s\n",
      "Iteration 100, Loss: 0.001877, T=4s, eta=44s\n",
      "Iteration 110, Loss: -0.635117, T=5s, eta=43s\n",
      "Iteration 120, Loss: -0.650893, T=5s, eta=43s\n",
      "Iteration 130, Loss: -0.663230, T=6s, eta=42s\n",
      "Iteration 140, Loss: -0.674812, T=6s, eta=42s\n",
      "Iteration 150, Loss: -0.679072, T=7s, eta=41s\n",
      "Iteration 160, Loss: -0.692299, T=7s, eta=40s\n",
      "Iteration 170, Loss: -0.702471, T=8s, eta=40s\n",
      "Iteration 180, Loss: -0.678471, T=8s, eta=39s\n",
      "Iteration 190, Loss: -0.497083, T=9s, eta=39s\n",
      "Iteration 200, Loss: -0.596486, T=9s, eta=38s\n",
      "Iteration 210, Loss: -0.687310, T=10s, eta=38s\n",
      "Iteration 220, Loss: -0.715051, T=10s, eta=37s\n",
      "Iteration 230, Loss: -0.720896, T=11s, eta=37s\n",
      "Iteration 240, Loss: -0.730772, T=11s, eta=36s\n",
      "Iteration 250, Loss: -0.732422, T=12s, eta=36s\n",
      "Iteration 260, Loss: -0.735856, T=12s, eta=35s\n",
      "Iteration 270, Loss: -0.738094, T=13s, eta=35s\n",
      "Iteration 280, Loss: -0.739973, T=13s, eta=34s\n",
      "Iteration 290, Loss: -0.741304, T=14s, eta=34s\n",
      "Iteration 300, Loss: -0.737775, T=14s, eta=33s\n",
      "Iteration 310, Loss: -0.741586, T=14s, eta=33s\n",
      "Iteration 320, Loss: -0.744434, T=15s, eta=32s\n",
      "Iteration 330, Loss: -0.745069, T=15s, eta=32s\n",
      "Iteration 340, Loss: 0.139373, T=16s, eta=31s\n",
      "Iteration 350, Loss: -0.740819, T=16s, eta=31s\n",
      "Iteration 360, Loss: -0.744765, T=17s, eta=30s\n",
      "Iteration 370, Loss: -0.733167, T=17s, eta=30s\n",
      "Iteration 380, Loss: -0.742995, T=18s, eta=29s\n",
      "Iteration 390, Loss: -0.716353, T=18s, eta=29s\n",
      "Iteration 400, Loss: -0.687185, T=19s, eta=28s\n",
      "Iteration 410, Loss: -0.734705, T=19s, eta=28s\n",
      "Iteration 420, Loss: -0.718688, T=20s, eta=27s\n",
      "Iteration 430, Loss: -0.747977, T=20s, eta=27s\n",
      "Iteration 440, Loss: -0.744421, T=21s, eta=26s\n",
      "Iteration 450, Loss: 0.418226, T=21s, eta=26s\n",
      "Iteration 460, Loss: -0.653053, T=21s, eta=25s\n",
      "Iteration 470, Loss: -0.646682, T=22s, eta=25s\n",
      "Iteration 480, Loss: -0.706825, T=22s, eta=24s\n",
      "Iteration 490, Loss: -0.739873, T=23s, eta=24s\n",
      "Iteration 500, Loss: -0.752611, T=23s, eta=23s\n",
      "Iteration 510, Loss: -0.698660, T=24s, eta=23s\n",
      "Iteration 520, Loss: -0.735250, T=24s, eta=22s\n",
      "Iteration 530, Loss: -0.748859, T=25s, eta=22s\n",
      "Iteration 540, Loss: -0.747789, T=25s, eta=21s\n",
      "Iteration 550, Loss: -0.744283, T=26s, eta=21s\n",
      "Iteration 560, Loss: -0.753094, T=26s, eta=20s\n",
      "Iteration 570, Loss: -0.747777, T=27s, eta=20s\n",
      "Iteration 580, Loss: -0.751371, T=27s, eta=20s\n",
      "Iteration 590, Loss: -0.755556, T=28s, eta=19s\n",
      "Iteration 600, Loss: 0.427076, T=28s, eta=19s\n",
      "Iteration 610, Loss: -0.754305, T=29s, eta=18s\n",
      "Iteration 620, Loss: -0.756657, T=29s, eta=18s\n",
      "Iteration 630, Loss: -0.756845, T=29s, eta=17s\n",
      "Iteration 640, Loss: -0.757479, T=30s, eta=17s\n",
      "Iteration 650, Loss: -0.757876, T=30s, eta=16s\n",
      "Iteration 660, Loss: -0.757892, T=31s, eta=16s\n",
      "Iteration 670, Loss: -0.729592, T=31s, eta=15s\n",
      "Iteration 680, Loss: -0.631033, T=32s, eta=15s\n",
      "Iteration 690, Loss: -0.603032, T=32s, eta=14s\n",
      "Iteration 700, Loss: 0.311760, T=33s, eta=14s\n",
      "Iteration 710, Loss: -0.492192, T=33s, eta=13s\n",
      "Iteration 720, Loss: 0.366116, T=34s, eta=13s\n",
      "Iteration 730, Loss: -0.395502, T=34s, eta=12s\n",
      "Iteration 740, Loss: 0.073281, T=35s, eta=12s\n",
      "Iteration 750, Loss: -0.467124, T=35s, eta=11s\n",
      "Iteration 760, Loss: -0.719841, T=36s, eta=11s\n",
      "Iteration 770, Loss: -0.729793, T=36s, eta=10s\n",
      "Iteration 780, Loss: -0.677960, T=37s, eta=10s\n",
      "Iteration 790, Loss: -0.665717, T=37s, eta=10s\n",
      "Iteration 800, Loss: -0.724761, T=37s, eta=9s\n",
      "Iteration 810, Loss: -0.761162, T=38s, eta=9s\n",
      "Iteration 820, Loss: -0.760719, T=38s, eta=8s\n",
      "Iteration 830, Loss: -0.755115, T=39s, eta=8s\n",
      "Iteration 840, Loss: -0.628045, T=39s, eta=7s\n",
      "Iteration 850, Loss: -0.762119, T=40s, eta=7s\n",
      "Iteration 860, Loss: -0.741197, T=40s, eta=6s\n",
      "Iteration 870, Loss: -0.761167, T=41s, eta=6s\n",
      "Iteration 880, Loss: -0.761848, T=41s, eta=5s\n",
      "Iteration 890, Loss: -0.763799, T=42s, eta=5s\n",
      "Iteration 900, Loss: -0.763930, T=42s, eta=4s\n",
      "Iteration 910, Loss: -0.764527, T=43s, eta=4s\n",
      "Iteration 920, Loss: -0.764796, T=43s, eta=3s\n",
      "Iteration 930, Loss: -0.765092, T=44s, eta=3s\n",
      "Iteration 940, Loss: -0.765368, T=44s, eta=2s\n",
      "Iteration 950, Loss: -0.759562, T=45s, eta=2s\n",
      "Iteration 960, Loss: -0.760525, T=45s, eta=1s\n",
      "Iteration 970, Loss: -0.765644, T=46s, eta=1s\n",
      "Iteration 980, Loss: -0.766282, T=46s, eta=0s\n",
      "Iteration 990, Loss: -0.766442, T=46s, eta=0s\n",
      "reward weights: (0, 1)\n",
      " final_fidelity: 0.004632685583877006\n",
      " final_purity: None\n",
      " purity_each_timestep: [1.00000001e+00 7.17251440e-03 8.03589668e-01 4.18494519e-04\n",
      " 4.21167555e-01 4.63268558e-03]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here the loss directly corressponds to the -fidelity (when converging) because log(1) is 0 and\n",
    "# the algorithm is choosing params that makes the POVM generate prob = 1\n",
    "from feedback_grape.fgrape import Gate, Decay\n",
    "\n",
    "measure = Gate(\n",
    "    gate=povm_measure_operator,\n",
    "    initial_params=[0.058, jnp.pi / 2],  # gamma and\n",
    "    measurement_flag=True,\n",
    ")\n",
    "\n",
    "qub_unitary = Gate(\n",
    "    gate=qubit_unitary,\n",
    "    initial_params=[jnp.pi / 3],  # alpha\n",
    "    measurement_flag=False,\n",
    "    # \"param_constraints\": [[0, 0.5], [-1, 1]],\n",
    ")\n",
    "\n",
    "qub_cav = Gate(\n",
    "    gate=qubit_cavity_unitary,\n",
    "    initial_params=[jnp.pi / 3],  # beta\n",
    "    measurement_flag=False,\n",
    "    # \"param_constraints\": [[0, 0.5], [-1, 1]],\n",
    ")\n",
    "\n",
    "\n",
    "system_params = [measure, qub_unitary, qub_cav]\n",
    "for reward_weights in [(1,1),(0,1)]:\n",
    "    result = optimize_pulse(\n",
    "        U_0=rho_target,\n",
    "        C_target=rho_target,\n",
    "        system_params=system_params,\n",
    "        num_time_steps=2,\n",
    "        reward_weights=reward_weights,\n",
    "        mode=\"lookup\",\n",
    "        goal=\"fidelity\",\n",
    "        max_iter=1000,\n",
    "        convergence_threshold=1e-16,\n",
    "        learning_rate=0.005,\n",
    "        evo_type=\"density\",\n",
    "        batch_size=1,\n",
    "        eval_time_steps=5,\n",
    "        progress=True\n",
    "    )\n",
    "\n",
    "    print(f\"reward weights: {reward_weights}\\n final_fidelity: {result.final_fidelity}\\n fidelity_each_timestep: {jnp.array(result.fidelity_each_timestep)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f410e7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([0.03819767, 2.39149717], dtype=float64),\n",
       " Array([1.04719755], dtype=float64),\n",
       " Array([1.04719755], dtype=float64)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.optimized_trainable_parameters[\"initial_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "09d6f7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Array([[0.03819767, 2.39149717],\n",
       "         [0.03819767, 2.39149717],\n",
       "         [0.03819767, 2.39149717],\n",
       "         [0.03819767, 2.39149717],\n",
       "         [0.03819767, 2.39149717],\n",
       "         [0.03819767, 2.39149717],\n",
       "         [0.03819767, 2.39149717],\n",
       "         [0.03819767, 2.39149717],\n",
       "         [0.03819767, 2.39149717],\n",
       "         [0.03819767, 2.39149717]], dtype=float64),\n",
       "  Array([[-2.78661739e-06],\n",
       "         [-2.78661739e-06],\n",
       "         [-2.78661739e-06],\n",
       "         [-2.78661739e-06],\n",
       "         [-2.78661739e-06],\n",
       "         [-2.78661739e-06],\n",
       "         [-2.78661739e-06],\n",
       "         [-2.78661739e-06],\n",
       "         [-2.78661739e-06],\n",
       "         [-2.78661739e-06]], dtype=float64),\n",
       "  Array([[0.94894431],\n",
       "         [0.94894431],\n",
       "         [0.94894431],\n",
       "         [0.94894431],\n",
       "         [0.94894431],\n",
       "         [0.94894431],\n",
       "         [0.94894431],\n",
       "         [0.94894431],\n",
       "         [0.94894431],\n",
       "         [0.94894431]], dtype=float64)],\n",
       " [Array([[0.05792869, 1.90432359],\n",
       "         [0.05792869, 1.90432359],\n",
       "         [0.05792869, 1.90432359],\n",
       "         [0.05792869, 1.90432359],\n",
       "         [0.05792869, 1.90432359],\n",
       "         [0.05792869, 1.90432359],\n",
       "         [0.05792869, 1.90432359],\n",
       "         [0.05792869, 1.90432359],\n",
       "         [0.05792869, 1.90432359],\n",
       "         [0.05792869, 1.90432359]], dtype=float64),\n",
       "  Array([[-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07]], dtype=float64),\n",
       "  Array([[1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633]], dtype=float64)],\n",
       " [Array([[0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633]], dtype=float64),\n",
       "  Array([[-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07]], dtype=float64),\n",
       "  Array([[1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633]], dtype=float64)],\n",
       " [Array([[0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633]], dtype=float64),\n",
       "  Array([[-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [ 1.32441198e-01]], dtype=float64),\n",
       "  Array([[1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.22197508]], dtype=float64)],\n",
       " [Array([[0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633]], dtype=float64),\n",
       "  Array([[-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [-2.11350413e-07],\n",
       "         [ 7.00843731e-01]], dtype=float64),\n",
       "  Array([[1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.01160633],\n",
       "         [1.20345246]], dtype=float64)]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.returned_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "546d79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial fidelity: 1.0000000093148405\n",
      "fidelity of state 0: 0.004827095072388107\n",
      "fidelity of state 1: 0.004827095072388107\n",
      "fidelity of state 2: 0.004827095072388107\n",
      "fidelity of state 3: 0.004827095072388107\n",
      "fidelity of state 4: 0.004827095072388107\n",
      "fidelity of state 5: 0.004827095072388107\n",
      "fidelity of state 6: 0.004827095072388107\n",
      "fidelity of state 7: 0.004827095072388107\n",
      "fidelity of state 8: 0.004827095072388107\n",
      "fidelity of state 9: 0.0028830001872770955\n"
     ]
    }
   ],
   "source": [
    "from feedback_grape.utils.fidelity import fidelity\n",
    "\n",
    "print(\n",
    "    \"initial fidelity:\",\n",
    "    fidelity(C_target=rho_target, U_final=rho_target, evo_type=\"density\"),\n",
    ")\n",
    "for i, state in enumerate(result.final_state):\n",
    "    print(\n",
    "        f\"fidelity of state {i}:\",\n",
    "        fidelity(C_target=rho_target, U_final=state, evo_type=\"density\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2190e32",
   "metadata": {},
   "source": [
    "### With Dissipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eec5fd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Loss: -0.398303, T=0s, eta=79s\n",
      "Iteration 20, Loss: -0.435848, T=1s, eta=82s\n",
      "Iteration 30, Loss: -0.442211, T=2s, eta=83s\n",
      "Iteration 40, Loss: -0.426085, T=3s, eta=83s\n",
      "Iteration 50, Loss: -0.388271, T=4s, eta=82s\n",
      "Iteration 60, Loss: 0.117361, T=5s, eta=81s\n",
      "Iteration 70, Loss: -0.227680, T=6s, eta=80s\n",
      "Iteration 80, Loss: -0.228944, T=7s, eta=80s\n",
      "Iteration 90, Loss: -0.233368, T=7s, eta=79s\n",
      "Iteration 100, Loss: -0.231770, T=8s, eta=78s\n",
      "Iteration 110, Loss: -0.233944, T=9s, eta=77s\n",
      "Iteration 120, Loss: 0.131030, T=10s, eta=76s\n",
      "Iteration 130, Loss: -0.231770, T=11s, eta=76s\n",
      "Iteration 140, Loss: -0.228728, T=12s, eta=75s\n",
      "Iteration 150, Loss: -0.206972, T=13s, eta=74s\n",
      "Iteration 160, Loss: 0.104981, T=14s, eta=73s\n",
      "Iteration 170, Loss: 0.072543, T=14s, eta=72s\n",
      "Iteration 180, Loss: 0.079497, T=15s, eta=71s\n",
      "Iteration 190, Loss: 0.124889, T=16s, eta=71s\n",
      "Iteration 200, Loss: -0.245274, T=17s, eta=70s\n",
      "Iteration 210, Loss: -0.247281, T=18s, eta=69s\n",
      "Iteration 220, Loss: -0.248583, T=19s, eta=68s\n",
      "Iteration 230, Loss: 0.395815, T=20s, eta=67s\n",
      "Iteration 240, Loss: -0.250181, T=21s, eta=66s\n",
      "Iteration 250, Loss: -0.244981, T=22s, eta=65s\n",
      "Iteration 260, Loss: 0.012338, T=22s, eta=65s\n",
      "Iteration 270, Loss: 0.015445, T=23s, eta=64s\n",
      "Iteration 280, Loss: -0.253026, T=24s, eta=63s\n",
      "Iteration 290, Loss: -0.013206, T=25s, eta=62s\n",
      "Iteration 300, Loss: -0.254132, T=26s, eta=61s\n",
      "Iteration 310, Loss: -0.023865, T=27s, eta=60s\n",
      "Iteration 320, Loss: -0.026432, T=28s, eta=59s\n",
      "Iteration 330, Loss: -0.027528, T=29s, eta=59s\n",
      "Iteration 340, Loss: -0.028132, T=30s, eta=58s\n",
      "Iteration 350, Loss: -0.246439, T=30s, eta=57s\n",
      "Iteration 360, Loss: -0.028543, T=31s, eta=56s\n",
      "Iteration 370, Loss: -0.184540, T=32s, eta=55s\n",
      "Iteration 380, Loss: -0.238544, T=33s, eta=54s\n",
      "Iteration 390, Loss: -0.026598, T=34s, eta=53s\n",
      "Iteration 400, Loss: -0.256658, T=35s, eta=53s\n",
      "Iteration 410, Loss: -0.028952, T=36s, eta=52s\n",
      "Iteration 420, Loss: -0.029232, T=37s, eta=51s\n",
      "Iteration 430, Loss: -0.029363, T=38s, eta=50s\n",
      "Iteration 440, Loss: -0.248413, T=39s, eta=49s\n",
      "Iteration 450, Loss: -0.254436, T=39s, eta=48s\n",
      "Iteration 460, Loss: -0.186942, T=40s, eta=47s\n",
      "Iteration 470, Loss: -0.017093, T=41s, eta=46s\n",
      "Iteration 480, Loss: 0.013311, T=42s, eta=46s\n",
      "Iteration 490, Loss: 0.452794, T=43s, eta=45s\n",
      "Iteration 500, Loss: 0.313547, T=44s, eta=44s\n",
      "Iteration 510, Loss: 0.267629, T=45s, eta=43s\n",
      "Iteration 520, Loss: -0.247152, T=46s, eta=42s\n",
      "Iteration 530, Loss: -0.246741, T=46s, eta=41s\n",
      "Iteration 540, Loss: -0.247991, T=47s, eta=40s\n",
      "Iteration 550, Loss: -0.248784, T=48s, eta=39s\n",
      "Iteration 560, Loss: -0.249056, T=49s, eta=39s\n",
      "Iteration 570, Loss: 0.384997, T=50s, eta=38s\n",
      "Iteration 580, Loss: -0.249011, T=51s, eta=37s\n",
      "Iteration 590, Loss: -0.224742, T=52s, eta=36s\n",
      "Iteration 600, Loss: 0.042893, T=53s, eta=35s\n",
      "Iteration 610, Loss: -0.015330, T=54s, eta=34s\n",
      "Iteration 620, Loss: -0.027327, T=55s, eta=33s\n",
      "Iteration 630, Loss: -0.234035, T=56s, eta=32s\n",
      "Iteration 640, Loss: -0.246627, T=56s, eta=32s\n",
      "Iteration 650, Loss: -0.250428, T=57s, eta=31s\n",
      "Iteration 660, Loss: -0.027901, T=58s, eta=30s\n",
      "Iteration 670, Loss: -0.249368, T=59s, eta=29s\n",
      "Iteration 680, Loss: -0.249601, T=60s, eta=28s\n",
      "Iteration 690, Loss: -0.250993, T=61s, eta=27s\n",
      "Iteration 700, Loss: -0.246393, T=62s, eta=26s\n",
      "Iteration 710, Loss: -0.246799, T=63s, eta=25s\n",
      "Iteration 720, Loss: -0.215344, T=64s, eta=25s\n",
      "Iteration 730, Loss: -0.235125, T=65s, eta=24s\n",
      "Iteration 740, Loss: -0.251594, T=66s, eta=23s\n",
      "Iteration 750, Loss: -0.020391, T=66s, eta=22s\n",
      "Iteration 760, Loss: -0.071366, T=68s, eta=21s\n",
      "Iteration 770, Loss: 0.202455, T=69s, eta=20s\n",
      "Iteration 780, Loss: -0.005007, T=70s, eta=19s\n",
      "Iteration 790, Loss: 0.226419, T=71s, eta=18s\n",
      "Iteration 800, Loss: 0.167540, T=72s, eta=18s\n",
      "Iteration 810, Loss: 0.175399, T=73s, eta=17s\n",
      "Iteration 820, Loss: -0.155799, T=74s, eta=16s\n",
      "Iteration 830, Loss: 0.689109, T=75s, eta=15s\n",
      "Iteration 840, Loss: -0.251737, T=75s, eta=14s\n",
      "Iteration 850, Loss: -0.251671, T=76s, eta=13s\n",
      "Iteration 860, Loss: -0.254206, T=77s, eta=12s\n",
      "Iteration 870, Loss: 0.213628, T=78s, eta=11s\n",
      "Iteration 880, Loss: 0.484666, T=79s, eta=10s\n",
      "Iteration 890, Loss: -0.199087, T=80s, eta=10s\n",
      "Iteration 900, Loss: -0.188903, T=81s, eta=9s\n",
      "Iteration 910, Loss: -0.233355, T=82s, eta=8s\n",
      "Iteration 920, Loss: -0.254702, T=83s, eta=7s\n",
      "Iteration 930, Loss: 0.474670, T=83s, eta=6s\n",
      "Iteration 940, Loss: 0.411558, T=84s, eta=5s\n",
      "Iteration 950, Loss: 0.273495, T=85s, eta=4s\n",
      "Iteration 960, Loss: -0.128828, T=86s, eta=3s\n",
      "Iteration 970, Loss: 0.320487, T=87s, eta=2s\n",
      "Iteration 980, Loss: 0.793685, T=88s, eta=1s\n",
      "Iteration 990, Loss: -0.001735, T=89s, eta=0s\n",
      "reward weights: (1, 1)\n",
      " final_fidelity: 0.03669946196542354\n",
      " final_purity: None\n",
      " fidelity_each_timestep: [1.00000001e+00 7.03624613e-04 5.86149802e-01 1.68815306e-02\n",
      " 2.64672067e-01 3.66994620e-02]\n",
      "\n",
      "Iteration 10, Loss: -0.399043, T=0s, eta=85s\n",
      "Iteration 20, Loss: -0.435469, T=1s, eta=90s\n",
      "Iteration 30, Loss: -0.457190, T=2s, eta=90s\n",
      "Iteration 40, Loss: -0.477772, T=3s, eta=93s\n",
      "Iteration 50, Loss: -0.449363, T=5s, eta=94s\n",
      "Iteration 60, Loss: -0.440361, T=5s, eta=92s\n",
      "Iteration 70, Loss: -0.468672, T=6s, eta=90s\n",
      "Iteration 80, Loss: -0.539256, T=7s, eta=89s\n",
      "Iteration 90, Loss: -0.554149, T=8s, eta=88s\n",
      "Iteration 100, Loss: -0.570740, T=9s, eta=87s\n",
      "Iteration 110, Loss: -0.559927, T=10s, eta=87s\n",
      "Iteration 120, Loss: -0.490841, T=11s, eta=86s\n",
      "Iteration 130, Loss: -0.489005, T=12s, eta=84s\n",
      "Iteration 140, Loss: -0.537240, T=13s, eta=83s\n",
      "Iteration 150, Loss: -0.596587, T=14s, eta=82s\n",
      "Iteration 160, Loss: -0.567435, T=15s, eta=80s\n",
      "Iteration 170, Loss: -0.549235, T=16s, eta=79s\n",
      "Iteration 180, Loss: -0.595704, T=17s, eta=78s\n",
      "Iteration 190, Loss: -0.549856, T=18s, eta=77s\n",
      "Iteration 200, Loss: -0.377677, T=19s, eta=76s\n",
      "Iteration 210, Loss: -0.262696, T=20s, eta=75s\n",
      "Iteration 220, Loss: -0.422396, T=20s, eta=74s\n",
      "Iteration 230, Loss: 0.161164, T=21s, eta=73s\n",
      "Iteration 240, Loss: -0.227098, T=22s, eta=72s\n",
      "Iteration 250, Loss: -0.518056, T=23s, eta=71s\n",
      "Iteration 260, Loss: -0.627943, T=24s, eta=69s\n",
      "Iteration 270, Loss: -0.652782, T=25s, eta=68s\n",
      "Iteration 280, Loss: -0.666407, T=26s, eta=67s\n",
      "Iteration 290, Loss: -0.669000, T=27s, eta=66s\n",
      "Iteration 300, Loss: -0.671265, T=28s, eta=66s\n",
      "Iteration 310, Loss: -0.673357, T=29s, eta=65s\n",
      "Iteration 320, Loss: -0.674611, T=30s, eta=64s\n",
      "Iteration 330, Loss: -0.675462, T=31s, eta=63s\n",
      "Iteration 340, Loss: -0.676194, T=32s, eta=62s\n",
      "Iteration 350, Loss: -0.676848, T=32s, eta=61s\n",
      "Iteration 360, Loss: -0.673358, T=33s, eta=60s\n",
      "Iteration 370, Loss: -0.652371, T=34s, eta=59s\n",
      "Iteration 380, Loss: -0.668288, T=35s, eta=58s\n",
      "Iteration 390, Loss: -0.676169, T=36s, eta=57s\n",
      "Iteration 400, Loss: -0.639405, T=37s, eta=56s\n",
      "Iteration 410, Loss: 0.434332, T=38s, eta=55s\n",
      "Iteration 420, Loss: -0.641338, T=39s, eta=54s\n",
      "Iteration 430, Loss: 0.379204, T=40s, eta=53s\n",
      "Iteration 440, Loss: -0.549193, T=41s, eta=52s\n",
      "Iteration 450, Loss: -0.313423, T=42s, eta=51s\n",
      "Iteration 460, Loss: -0.134239, T=43s, eta=50s\n",
      "Iteration 470, Loss: -0.405379, T=44s, eta=49s\n",
      "Iteration 480, Loss: -0.587026, T=45s, eta=48s\n",
      "Iteration 490, Loss: -0.561507, T=45s, eta=47s\n",
      "Iteration 500, Loss: 0.470828, T=46s, eta=46s\n",
      "Iteration 510, Loss: -0.635887, T=47s, eta=45s\n",
      "Iteration 520, Loss: -0.672114, T=48s, eta=44s\n",
      "Iteration 530, Loss: -0.678992, T=49s, eta=44s\n",
      "Iteration 540, Loss: -0.681934, T=50s, eta=43s\n",
      "Iteration 550, Loss: -0.681089, T=51s, eta=42s\n",
      "Iteration 560, Loss: -0.682653, T=52s, eta=41s\n",
      "Iteration 570, Loss: -0.682854, T=53s, eta=40s\n",
      "Iteration 580, Loss: -0.680925, T=54s, eta=39s\n",
      "Iteration 590, Loss: -0.668849, T=55s, eta=38s\n",
      "Iteration 600, Loss: -0.668164, T=56s, eta=37s\n",
      "Iteration 610, Loss: -0.665702, T=57s, eta=36s\n",
      "Iteration 620, Loss: -0.679184, T=58s, eta=35s\n",
      "Iteration 630, Loss: -0.656457, T=58s, eta=34s\n",
      "Iteration 640, Loss: -0.674870, T=59s, eta=33s\n",
      "Iteration 650, Loss: -0.684342, T=60s, eta=32s\n",
      "Iteration 660, Loss: -0.685067, T=61s, eta=31s\n",
      "Iteration 670, Loss: -0.681426, T=62s, eta=30s\n",
      "Iteration 680, Loss: -0.657581, T=63s, eta=29s\n",
      "Iteration 690, Loss: -0.658211, T=64s, eta=28s\n",
      "Iteration 700, Loss: -0.657289, T=65s, eta=28s\n",
      "Iteration 710, Loss: -0.677043, T=66s, eta=27s\n",
      "Iteration 720, Loss: -0.670598, T=67s, eta=26s\n",
      "Iteration 730, Loss: -0.682221, T=68s, eta=25s\n",
      "Iteration 740, Loss: -0.687565, T=68s, eta=24s\n",
      "Iteration 750, Loss: -0.682703, T=69s, eta=23s\n",
      "Iteration 760, Loss: -0.647452, T=70s, eta=22s\n",
      "Iteration 770, Loss: 0.742430, T=71s, eta=21s\n",
      "Iteration 780, Loss: -0.603770, T=72s, eta=20s\n",
      "Iteration 790, Loss: -0.633164, T=73s, eta=19s\n",
      "Iteration 800, Loss: -0.673287, T=74s, eta=18s\n",
      "Iteration 810, Loss: 0.248264, T=75s, eta=17s\n",
      "Iteration 820, Loss: -0.683578, T=76s, eta=16s\n",
      "Iteration 830, Loss: -0.672348, T=77s, eta=15s\n",
      "Iteration 840, Loss: -0.683905, T=77s, eta=14s\n",
      "Iteration 850, Loss: -0.690128, T=78s, eta=13s\n",
      "Iteration 860, Loss: 0.145006, T=79s, eta=13s\n",
      "Iteration 870, Loss: -0.689545, T=80s, eta=12s\n",
      "Iteration 880, Loss: -0.690067, T=81s, eta=11s\n",
      "Iteration 890, Loss: -0.691329, T=82s, eta=10s\n",
      "Iteration 900, Loss: -0.687371, T=83s, eta=9s\n",
      "Iteration 910, Loss: -0.663954, T=84s, eta=8s\n",
      "Iteration 920, Loss: -0.678393, T=85s, eta=7s\n",
      "Iteration 930, Loss: -0.672444, T=86s, eta=6s\n",
      "Iteration 940, Loss: -0.687457, T=87s, eta=5s\n",
      "Iteration 950, Loss: -0.692922, T=87s, eta=4s\n",
      "Iteration 960, Loss: -0.692762, T=88s, eta=3s\n",
      "Iteration 970, Loss: -0.693201, T=89s, eta=2s\n",
      "Iteration 980, Loss: -0.690909, T=90s, eta=1s\n",
      "Iteration 990, Loss: -0.691543, T=91s, eta=1s\n",
      "reward weights: (0, 1)\n",
      " final_fidelity: 0.09989977847322556\n",
      " final_purity: None\n",
      " fidelity_each_timestep: [1.00000001e+00 7.97559054e-04 6.78304485e-01 1.05128159e-02\n",
      " 1.42811629e-01 9.98997785e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note if tsave = jnp.linspace(0, 1, 1) = [0.0] then the decay is not applied ?\n",
    "# because the first time step has the original non decayed state\n",
    "# Question: Here I should not use any sort of Hamiltonian or tspan or something?\n",
    "\n",
    "decay = Decay(\n",
    "    c_ops=[tensor(identity(N_cav), jnp.sqrt(0.10) * sigmam())],\n",
    ")\n",
    "system_params = [decay, measure, qub_unitary, qub_cav]\n",
    "for reward_weights in [(1,1),(0,1)]:\n",
    "    result = optimize_pulse(\n",
    "        U_0=rho_target,\n",
    "        C_target=rho_target,\n",
    "        system_params=system_params,\n",
    "        num_time_steps=2,\n",
    "        reward_weights=reward_weights,\n",
    "        mode=\"lookup\",\n",
    "        goal=\"fidelity\",\n",
    "        max_iter=1000,\n",
    "        convergence_threshold=1e-6,\n",
    "        learning_rate=0.005,\n",
    "        evo_type=\"density\",\n",
    "        batch_size=1,\n",
    "        eval_time_steps=5,\n",
    "        progress=True\n",
    "    )\n",
    "\n",
    "    print(f\"reward weights: {reward_weights}\\n final_fidelity: {result.final_fidelity}\\n fidelity_each_timestep: {jnp.array(result.fidelity_each_timestep)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3419d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09989977847322556\n",
      "[1.00000001e+00 7.97559054e-04 6.78304485e-01 1.05128159e-02\n",
      " 1.42811629e-01 9.98997785e-02]\n"
     ]
    }
   ],
   "source": [
    "print(result.final_fidelity)\n",
    "print(jnp.array(result.fidelity_each_timestep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "20bb8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial fidelity: 1.0000000093148405\n",
      "fidelity of state 0: 0.001521807302458907\n",
      "fidelity of state 1: 0.137835456053904\n",
      "fidelity of state 2: 0.019578787673703902\n",
      "fidelity of state 3: 0.137835456053904\n",
      "fidelity of state 4: 0.137835456053904\n",
      "fidelity of state 5: 0.137835456053904\n",
      "fidelity of state 6: 0.137835456053904\n",
      "fidelity of state 7: 0.137835456053904\n",
      "fidelity of state 8: 0.013048997378764634\n",
      "fidelity of state 9: 0.137835456053904\n"
     ]
    }
   ],
   "source": [
    "from feedback_grape.utils.fidelity import fidelity\n",
    "\n",
    "print(\n",
    "    \"initial fidelity:\",\n",
    "    fidelity(C_target=rho_target, U_final=rho_target, evo_type=\"density\"),\n",
    ")\n",
    "for i, state in enumerate(result.final_state):\n",
    "    print(\n",
    "        f\"fidelity of state {i}:\",\n",
    "        fidelity(C_target=rho_target, U_final=state, evo_type=\"density\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "51c5f007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Array([[0.0376334 , 2.39761838],\n",
       "         [0.0376334 , 2.39761838],\n",
       "         [0.0376334 , 2.39761838],\n",
       "         [0.0376334 , 2.39761838],\n",
       "         [0.0376334 , 2.39761838],\n",
       "         [0.0376334 , 2.39761838],\n",
       "         [0.0376334 , 2.39761838],\n",
       "         [0.0376334 , 2.39761838],\n",
       "         [0.0376334 , 2.39761838],\n",
       "         [0.0376334 , 2.39761838]], dtype=float64),\n",
       "  Array([[-5.88306427e-05],\n",
       "         [-5.88306427e-05],\n",
       "         [-5.88306427e-05],\n",
       "         [-5.88306427e-05],\n",
       "         [-5.88306427e-05],\n",
       "         [-5.88306427e-05],\n",
       "         [-5.88306427e-05],\n",
       "         [-5.88306427e-05],\n",
       "         [-5.88306427e-05],\n",
       "         [-5.88306427e-05]], dtype=float64),\n",
       "  Array([[0.98502685],\n",
       "         [0.98502685],\n",
       "         [0.98502685],\n",
       "         [0.98502685],\n",
       "         [0.98502685],\n",
       "         [0.98502685],\n",
       "         [0.98502685],\n",
       "         [0.98502685],\n",
       "         [0.98502685],\n",
       "         [0.98502685]], dtype=float64)],\n",
       " [Array([[-0.06107005,  1.36142286],\n",
       "         [-0.06107005,  1.36142286],\n",
       "         [-0.06107005,  1.36142286],\n",
       "         [-0.06107005,  1.36142286],\n",
       "         [-0.06107005,  1.36142286],\n",
       "         [-0.06107005,  1.36142286],\n",
       "         [-0.06107005,  1.36142286],\n",
       "         [-0.06107005,  1.36142286],\n",
       "         [-0.06107005,  1.36142286],\n",
       "         [-0.06107005,  1.36142286]], dtype=float64),\n",
       "  Array([[-3.95931237e-06],\n",
       "         [-3.95931237e-06],\n",
       "         [ 1.16323429e-01],\n",
       "         [-3.95931237e-06],\n",
       "         [-3.95931237e-06],\n",
       "         [-3.95931237e-06],\n",
       "         [-3.95931237e-06],\n",
       "         [-3.95931237e-06],\n",
       "         [-3.95931237e-06],\n",
       "         [-3.95931237e-06]], dtype=float64),\n",
       "  Array([[0.97498791],\n",
       "         [0.97498791],\n",
       "         [1.33778647],\n",
       "         [0.97498791],\n",
       "         [0.97498791],\n",
       "         [0.97498791],\n",
       "         [0.97498791],\n",
       "         [0.97498791],\n",
       "         [0.97498791],\n",
       "         [0.97498791]], dtype=float64)],\n",
       " [Array([[0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633]], dtype=float64),\n",
       "  Array([[ 9.37689542e-01],\n",
       "         [ 9.37689542e-01],\n",
       "         [-3.95931237e-06],\n",
       "         [ 9.37689542e-01],\n",
       "         [ 9.37689542e-01],\n",
       "         [ 9.37689542e-01],\n",
       "         [ 9.37689542e-01],\n",
       "         [ 9.37689542e-01],\n",
       "         [ 9.37689542e-01],\n",
       "         [ 9.37689542e-01]], dtype=float64),\n",
       "  Array([[1.11081645],\n",
       "         [1.11081645],\n",
       "         [0.97498791],\n",
       "         [1.11081645],\n",
       "         [1.11081645],\n",
       "         [1.11081645],\n",
       "         [1.11081645],\n",
       "         [1.11081645],\n",
       "         [1.11081645],\n",
       "         [1.11081645]], dtype=float64)],\n",
       " [Array([[0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633]], dtype=float64),\n",
       "  Array([[ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 9.37689542e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [-3.95931237e-06],\n",
       "         [ 1.16323429e-01]], dtype=float64),\n",
       "  Array([[1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.11081645],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [0.97498791],\n",
       "         [1.33778647]], dtype=float64)],\n",
       " [Array([[0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633],\n",
       "         [0.058     , 1.57079633]], dtype=float64),\n",
       "  Array([[-3.95931237e-06],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 1.16323429e-01],\n",
       "         [ 9.37689542e-01],\n",
       "         [ 1.16323429e-01]], dtype=float64),\n",
       "  Array([[0.97498791],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.33778647],\n",
       "         [1.11081645],\n",
       "         [1.33778647]], dtype=float64)]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.returned_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c4441f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([0.0376334 , 2.39761838], dtype=float64),\n",
       " Array([1.04719755], dtype=float64),\n",
       " Array([1.04719755], dtype=float64)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.optimized_trainable_parameters[\"initial_params\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
