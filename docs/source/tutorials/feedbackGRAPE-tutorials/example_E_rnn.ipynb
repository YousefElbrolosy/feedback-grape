{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16070338",
   "metadata": {},
   "source": [
    "# E: State stabilization with SNAP gates and displacement gates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e400e",
   "metadata": {},
   "source": [
    "The use of feedback GRAPE applied to the Jaynes-\n",
    "Cummings scenario allows us to discover strategies\n",
    "extending the lifetime of a range of quantum states. How-\n",
    "ever, for more complex quantum states such as kitten\n",
    "states, the infidelity becomes significant after just a few\n",
    "dissipative evolution steps in spite of the feedback [cf.\n",
    "Fig. 6(c)]. This raises the question of whether the limited\n",
    "quality of the stabilization is to be attributed to a failure\n",
    "of our feedback-GRAPE learning algorithm to properly\n",
    "explore the control-parameter landscape or, rather, to the\n",
    "limited expressivity of the controls. With the goal of\n",
    "addressing this question, we test our method on the state-\n",
    "stabilization task using a more expressive control scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003222d",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import os\n",
    "\n",
    "os.sys.path.append(\"../../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "from feedback_grape.fgrape import optimize_pulse_with_feedback\n",
    "from feedback_grape.utils.operators import sigmam, identity, cosm, sinm\n",
    "from feedback_grape.utils.states import coherent, basis\n",
    "from feedback_grape.utils.tensor import tensor\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax.scipy.linalg import expm\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851f72b",
   "metadata": {},
   "source": [
    "## Initialize states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa672e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.fidelity import ket2dm\n",
    "\n",
    "N_cav = 30  # number of cavity modes\n",
    "N_snap = 15\n",
    "\n",
    "alpha = 2\n",
    "psi_target = coherent(N_cav, alpha) + coherent(N_cav, -alpha)\n",
    "\n",
    "# Normalize psi_target before constructing rho_target\n",
    "psi_target = psi_target / jnp.linalg.norm(psi_target)\n",
    "\n",
    "rho_target = ket2dm(psi_target)\n",
    "\n",
    "rho_target = tensor(rho_target, ket2dm(basis(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "702387d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity Operator\n",
    "from feedback_grape.utils.operators import create, destroy\n",
    "\n",
    "\n",
    "def parity_operator(N_cav):\n",
    "    return tensor(\n",
    "        jax.scipy.linalg.expm(1j * jnp.pi * (create(N_cav) @ destroy(N_cav))),\n",
    "        identity(2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa1b85c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parity check for the kitten2 state: True\n",
      "parity_check trace : 1.0000000000000013\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the kitten2 state has an even parity\n",
    "parity_op = parity_operator(N_cav)\n",
    "parity_check = jnp.isclose(\n",
    "    jnp.trace((parity_op @ rho_target) @ rho_target), 1.0\n",
    ")\n",
    "print(\"Parity check for the kitten2 state:\", parity_check)\n",
    "print(\"parity_check trace :\", jnp.real(jnp.trace(parity_op @ rho_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eda36a",
   "metadata": {},
   "source": [
    "## Initialize the parameterized Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9381af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displacement_gate(alpha_re, alpha_im):\n",
    "    \"\"\"Displacement operator for a coherent state.\"\"\"\n",
    "    alpha = alpha_re + 1j * alpha_im\n",
    "    gate = jax.scipy.linalg.expm(\n",
    "        alpha * create(N_cav) - alpha.conj() * destroy(N_cav)\n",
    "    )\n",
    "    return tensor(gate, identity(2))\n",
    "\n",
    "\n",
    "def displacement_gate_dag(alpha_re, alpha_im):\n",
    "    \"\"\"Displacement operator for a coherent state.\"\"\"\n",
    "    alpha = alpha_re + 1j * alpha_im\n",
    "    gate = (\n",
    "        jax.scipy.linalg.expm(\n",
    "            alpha * create(N_cav) - alpha.conj() * destroy(N_cav)\n",
    "        )\n",
    "        .conj()\n",
    "        .T\n",
    "    )\n",
    "    return tensor(gate, identity(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d692ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO/ QUESTION: see whether to remove the dereferencing thing or keep it that way?\n",
    "def snap_gate(\n",
    "    phase0,\n",
    "    phase1,\n",
    "    phase2,\n",
    "    phase3,\n",
    "    phase4,\n",
    "    phase5,\n",
    "    phase6,\n",
    "    phase7,\n",
    "    phase8,\n",
    "    phase9,\n",
    "    phase10,\n",
    "    phase11,\n",
    "    phase12,\n",
    "    phase13,\n",
    "    phase14,\n",
    "):\n",
    "    phase_list = [\n",
    "        phase0,\n",
    "        phase1,\n",
    "        phase2,\n",
    "        phase3,\n",
    "        phase4,\n",
    "        phase5,\n",
    "        phase6,\n",
    "        phase7,\n",
    "        phase8,\n",
    "        phase9,\n",
    "        phase10,\n",
    "        phase11,\n",
    "        phase12,\n",
    "        phase13,\n",
    "        phase14,\n",
    "    ]\n",
    "    diags = jnp.ones(shape=(N_cav - len(phase_list)))\n",
    "    exponentiated = jnp.exp(1j * jnp.array(phase_list))\n",
    "    diags = jnp.concatenate((exponentiated, diags))\n",
    "    return tensor(jnp.diag(diags), identity(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a84b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.operators import create, destroy\n",
    "\n",
    "\n",
    "def povm_measure_operator(measurement_outcome, gamma, delta):\n",
    "    \"\"\"\n",
    "    POVM for the measurement of the cavity state.\n",
    "    returns Mm ( NOT the POVM element Em = Mm_dag @ Mm ), given measurement_outcome m, gamma and delta\n",
    "    \"\"\"\n",
    "    number_operator = tensor(create(N_cav) @ destroy(N_cav), identity(2))\n",
    "    angle = (gamma * number_operator) + delta / 2\n",
    "    meas_op = jnp.where(\n",
    "        measurement_outcome == 1,\n",
    "        cosm(angle),\n",
    "        sinm(angle),\n",
    "    )\n",
    "    return meas_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d72e6",
   "metadata": {},
   "source": [
    "## Initialize RNN of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83584086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "\n",
    "# You can do whatever you want inside so long as you maintaing the hidden_size and output size shapes\n",
    "class RNN(nn.Module):\n",
    "    hidden_size: int  # number of features in the hidden state\n",
    "    output_size: int  # number of features in the output ( 2 in the case of gamma and beta)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, measurement, hidden_state):\n",
    "        \"\"\"\n",
    "        If your GRU has a hidden state increasing number of features in the hidden stateH means:\n",
    "\n",
    "        - You're allowing the model to store more information across time steps\n",
    "\n",
    "        - Each time step can represent more complex features, patterns, or dependencies\n",
    "\n",
    "        - You're giving the GRU more representational capacity\n",
    "        \"\"\"\n",
    "        gru_cell = nn.GRUCell(\n",
    "            features=self.hidden_size,\n",
    "            gate_fn=nn.sigmoid,\n",
    "            activation_fn=nn.tanh,\n",
    "        )\n",
    "        self.make_rng('dropout')\n",
    "\n",
    "        if measurement.ndim == 1:\n",
    "            measurement = measurement.reshape(1, -1)\n",
    "\n",
    "        new_hidden_state, _ = gru_cell(hidden_state, measurement)\n",
    "        new_hidden_state = nn.Dropout(rate=0.1, deterministic=False)(\n",
    "            new_hidden_state\n",
    "        )\n",
    "        # this returns the povm_params after linear regression through the hidden state which contains\n",
    "        # the information of the previous time steps and this is optimized to output best povm_params\n",
    "        # new_hidden_state = nn.Dense(features=self.hidden_size)(new_hidden_state)\n",
    "        new_hidden_state = nn.Dense(\n",
    "            features=self.hidden_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "        )(new_hidden_state)\n",
    "        new_hidden_state = nn.relu(new_hidden_state)\n",
    "        new_hidden_state = nn.Dense(\n",
    "            features=self.hidden_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "        )(new_hidden_state)\n",
    "        new_hidden_state = nn.relu(new_hidden_state)\n",
    "        output = nn.Dense(\n",
    "            features=self.output_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "            bias_init=nn.initializers.constant(0.1),\n",
    "        )(new_hidden_state)\n",
    "        output = nn.relu(output)\n",
    "        # output = jnp.asarray(output)\n",
    "        return output[0], new_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846472f",
   "metadata": {},
   "source": [
    "### In this notebook, we decreased the convergence threshold and evaluate for num_time_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f91cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.051286\n",
      "Iteration 10, Loss: -0.046407\n",
      "Iteration 20, Loss: -0.289864\n",
      "Iteration 30, Loss: -0.796614\n",
      "Iteration 40, Loss: -0.857959\n",
      "Iteration 50, Loss: -0.955282\n",
      "Iteration 60, Loss: -0.748941\n",
      "Iteration 70, Loss: -0.908621\n",
      "Iteration 80, Loss: -0.785943\n",
      "Iteration 90, Loss: -0.416823\n",
      "Iteration 100, Loss: -0.676020\n",
      "Iteration 110, Loss: -0.240745\n",
      "Iteration 120, Loss: -0.233151\n",
      "Iteration 130, Loss: -0.682477\n",
      "Iteration 140, Loss: -0.281225\n",
      "Iteration 150, Loss: -0.315091\n",
      "Iteration 160, Loss: -0.413254\n",
      "Iteration 170, Loss: -0.227406\n",
      "Iteration 180, Loss: -0.266969\n",
      "Iteration 190, Loss: -0.964452\n",
      "Iteration 200, Loss: -0.973934\n",
      "Iteration 210, Loss: -0.555330\n",
      "Iteration 220, Loss: -0.968880\n",
      "Iteration 230, Loss: -0.779439\n",
      "Iteration 240, Loss: -0.953672\n",
      "Iteration 250, Loss: -0.983785\n",
      "Iteration 260, Loss: -0.958988\n",
      "Iteration 270, Loss: -0.615746\n",
      "Iteration 280, Loss: -0.330535\n",
      "Iteration 290, Loss: -0.257443\n",
      "Iteration 300, Loss: -0.659959\n",
      "Iteration 310, Loss: -0.798333\n",
      "Iteration 320, Loss: -0.441607\n",
      "Iteration 330, Loss: -0.652960\n",
      "Iteration 340, Loss: -0.982149\n",
      "Iteration 350, Loss: -0.981717\n",
      "Iteration 360, Loss: -0.989132\n",
      "Iteration 370, Loss: -0.988968\n",
      "Iteration 380, Loss: -0.989530\n",
      "Iteration 390, Loss: -0.989578\n",
      "Iteration 400, Loss: -0.989844\n",
      "Iteration 410, Loss: -0.989695\n",
      "Iteration 420, Loss: -0.989864\n"
     ]
    }
   ],
   "source": [
    "# Note if tsave = jnp.linspace(0, 1, 1) = [0.0] then the decay is not applied ?\n",
    "# because the first time step has the original non decayed state\n",
    "key = jax.random.PRNGKey(42)\n",
    "snap_init = jax.random.uniform(\n",
    "    key, shape=(N_snap,), minval=-jnp.pi, maxval=jnp.pi\n",
    ")\n",
    "# TODO/QUESTION: In documentation, clarify that the initial_params are the params up to the\n",
    "# point where measurement occurs, compared with other modes where the initial_params\n",
    "# are the initial params for the entire system for all time steps.\n",
    "measure = {\n",
    "    \"gate\": povm_measure_operator,\n",
    "    \"initial_params\": jax.random.uniform(\n",
    "        key,\n",
    "        shape=(1, 2),  # 2 for gamma and delta\n",
    "        minval=-jnp.pi,\n",
    "        maxval=jnp.pi,\n",
    "    )[0].tolist(),\n",
    "    \"measurement_flag\": True,\n",
    "    # \"param_constraints\": [[0, 0.5], [-1, 1]],\n",
    "}\n",
    "\n",
    "displacement = {\n",
    "    \"gate\": displacement_gate,\n",
    "    \"initial_params\": jax.random.uniform(\n",
    "        key, shape=(1, 2), minval=-jnp.pi, maxval=jnp.pi\n",
    "    )[0].tolist(),\n",
    "    \"measurement_flag\": False,\n",
    "}\n",
    "\n",
    "snap = {\n",
    "    \"gate\": snap_gate,\n",
    "    \"initial_params\": snap_init.tolist(),\n",
    "    \"measurement_flag\": False,\n",
    "}\n",
    "\n",
    "displacement_dag = {\n",
    "    \"gate\": displacement_gate_dag,\n",
    "    \"initial_params\": jax.random.uniform(\n",
    "        key, shape=(1, 2), minval=-jnp.pi, maxval=jnp.pi\n",
    "    )[0].tolist(),\n",
    "    \"measurement_flag\": False,\n",
    "}\n",
    "\n",
    "system_params = [measure, displacement, snap, displacement_dag]\n",
    "\n",
    "\n",
    "result = optimize_pulse_with_feedback(\n",
    "    U_0=rho_target,\n",
    "    C_target=rho_target,\n",
    "    decay={\n",
    "        \"decay_indices\": [0, 1],  # indices of gates before which decay occurs\n",
    "        \"c_ops\": {\n",
    "            \"tm\": [tensor(identity(N_cav), jnp.sqrt(0.005) * sigmam())],\n",
    "            \"tc\": [tensor(identity(N_cav), jnp.sqrt(0.005) * sigmam())],\n",
    "        },\n",
    "        \"tsave\": jnp.linspace(0, 1, 2),  # time grid for decay\n",
    "        \"Hamiltonian\": None,\n",
    "    },\n",
    "    system_params=system_params,\n",
    "    num_time_steps=2,\n",
    "    mode=\"nn\",\n",
    "    goal=\"fidelity\",\n",
    "    max_iter=1000,\n",
    "    convergence_threshold=1e-6,\n",
    "    learning_rate=0.09,\n",
    "    evo_type=\"density\",\n",
    "    batch_size=16,\n",
    "    rnn=RNN,\n",
    "    rnn_hidden_size=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "403a1197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.98983774, dtype=float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.final_fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24452f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60, 60)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.final_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8321286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.fidelity import ket2dm\n",
    "\n",
    "N_cav = 30  # number of cavity modes\n",
    "N_snap = 15\n",
    "\n",
    "alpha = 2\n",
    "psi_target = coherent(N_cav, alpha) + coherent(N_cav, -alpha)\n",
    "\n",
    "# Normalize psi_target before constructing rho_target\n",
    "psi_target = psi_target / jnp.linalg.norm(psi_target)\n",
    "\n",
    "rho_target = ket2dm(psi_target)\n",
    "\n",
    "rho_target = tensor(rho_target, ket2dm(basis(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "712ec5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial fidelity: 1.0000000191119465\n",
      "fidelity of state 0: 0.9896874628446899\n",
      "fidelity of state 1: 0.9896897540287319\n",
      "fidelity of state 2: 0.9894047072413638\n",
      "fidelity of state 3: 0.9898964331514902\n",
      "fidelity of state 4: 0.98981801110817\n",
      "fidelity of state 5: 0.9899699879774597\n",
      "fidelity of state 6: 0.9899626413311895\n",
      "fidelity of state 7: 0.9900407902354398\n",
      "fidelity of state 8: 0.9900467965501678\n",
      "fidelity of state 9: 0.9898608063615215\n"
     ]
    }
   ],
   "source": [
    "from feedback_grape.utils.fidelity import fidelity\n",
    "\n",
    "print(\n",
    "    \"initial fidelity:\",\n",
    "    fidelity(C_target=rho_target, U_final=rho_target, evo_type=\"density\"),\n",
    ")\n",
    "for i, state in enumerate(result.final_state):\n",
    "    print(\n",
    "        f\"fidelity of state {i}:\",\n",
    "        fidelity(C_target=rho_target, U_final=state, evo_type=\"density\"),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
