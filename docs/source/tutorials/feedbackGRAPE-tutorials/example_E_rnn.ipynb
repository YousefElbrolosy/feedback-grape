{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16070338",
   "metadata": {},
   "source": [
    "# E: State stabilization with SNAP gates and displacement gates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e400e",
   "metadata": {},
   "source": [
    "The use of feedback GRAPE applied to the Jaynes-\n",
    "Cummings scenario allows us to discover strategies\n",
    "extending the lifetime of a range of quantum states. How-\n",
    "ever, for more complex quantum states such as kitten\n",
    "states, the infidelity becomes significant after just a few\n",
    "dissipative evolution steps in spite of the feedback [cf.\n",
    "Fig. 6(c)]. This raises the question of whether the limited\n",
    "quality of the stabilization is to be attributed to a failure\n",
    "of our feedback-GRAPE learning algorithm to properly\n",
    "explore the control-parameter landscape or, rather, to the\n",
    "limited expressivity of the controls. With the goal of\n",
    "addressing this question, we test our method on the state-\n",
    "stabilization task using a more expressive control scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6003222d",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "import os\n",
    "\n",
    "os.sys.path.append(\"../../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cea5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "from feedback_grape.fgrape import optimize_pulse\n",
    "from feedback_grape.utils.operators import cosm, sinm, identity\n",
    "from feedback_grape.utils.states import coherent\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851f72b",
   "metadata": {},
   "source": [
    "## Initialize states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa672e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.fidelity import ket2dm\n",
    "\n",
    "N_cav = 30  # number of cavity modes\n",
    "N_snap = 15\n",
    "\n",
    "alpha = 2\n",
    "psi_target = coherent(N_cav, alpha) + coherent(N_cav, -alpha)\n",
    "\n",
    "# Normalize psi_target before constructing rho_target\n",
    "psi_target = psi_target / jnp.linalg.norm(psi_target)\n",
    "\n",
    "rho_target = ket2dm(psi_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702387d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity Operator\n",
    "from feedback_grape.utils.operators import create, destroy\n",
    "\n",
    "\n",
    "def parity_operator(N_cav):\n",
    "    return jax.scipy.linalg.expm(1j * jnp.pi * (create(N_cav) @ destroy(N_cav)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1b85c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parity check for the kitten2 state: True\n",
      "parity_check trace : 0.9999999999999994\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the kitten2 state has an even parity\n",
    "parity_op = parity_operator(N_cav)\n",
    "parity_check = jnp.isclose(\n",
    "    jnp.trace((parity_op @ rho_target) @ rho_target), 1.0\n",
    ")\n",
    "print(\"Parity check for the kitten2 state:\", parity_check)\n",
    "print(\"parity_check trace :\", jnp.real(jnp.trace(parity_op @ rho_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eda36a",
   "metadata": {},
   "source": [
    "## Initialize the parameterized Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9381af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displacement_gate(alphas):\n",
    "    \"\"\"Displacement operator for a coherent state.\"\"\"\n",
    "    alpha_re, alpha_im = alphas\n",
    "    alpha = alpha_re + 1j * alpha_im\n",
    "    gate = jax.scipy.linalg.expm(\n",
    "        alpha * create(N_cav) - alpha.conj() * destroy(N_cav)\n",
    "    )\n",
    "    return gate\n",
    "\n",
    "\n",
    "def displacement_gate_dag(alphas):\n",
    "    \"\"\"Displacement operator for a coherent state.\"\"\"\n",
    "    alpha_re, alpha_im = alphas\n",
    "    alpha = alpha_re + 1j * alpha_im\n",
    "    gate = (\n",
    "        jax.scipy.linalg.expm(\n",
    "            alpha * create(N_cav) - alpha.conj() * destroy(N_cav)\n",
    "        )\n",
    "        .conj()\n",
    "        .T\n",
    "    )\n",
    "    return gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d692ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_gate(phase_list):\n",
    "    diags = jnp.ones(shape=(N_cav - len(phase_list)))\n",
    "    exponentiated = jnp.exp(1j * jnp.array(phase_list))\n",
    "    diags = jnp.concatenate((exponentiated, diags))\n",
    "    return jnp.diag(diags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029b826",
   "metadata": {},
   "source": [
    "### povm_measure_operator (callable): <br>\n",
    "    - It should take a measurement outcome and list of params as input\n",
    "    - The measurement outcome options are either 1 or -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a84b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.operators import create, destroy\n",
    "\n",
    "\n",
    "def povm_measure_operator(measurement_outcome, params):\n",
    "    \"\"\"\n",
    "    POVM for the measurement of the cavity state.\n",
    "    returns Mm ( NOT the POVM element Em = Mm_dag @ Mm ), given measurement_outcome m, gamma and delta\n",
    "    \"\"\"\n",
    "    gamma, delta = params\n",
    "    angle = gamma * create(N_cav) @ destroy(N_cav) + delta / 2 * identity(N_cav)\n",
    "    meas_op = jnp.where(\n",
    "        measurement_outcome == 1,\n",
    "        cosm(angle),\n",
    "        sinm(angle),\n",
    "    )\n",
    "    return meas_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d72e6",
   "metadata": {},
   "source": [
    "## Initialize RNN of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83584086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "\n",
    "\n",
    "# You can do whatever you want inside so long as you maintaing the hidden_size and output size shapes\n",
    "class RNN(nn.Module):\n",
    "    hidden_size: int  # number of features in the hidden state\n",
    "    output_size: int  # number of features in the output (inferred from the number of parameters) just provide those attributes to the class\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, measurement, hidden_state):\n",
    "\n",
    "        if measurement.ndim == 1:\n",
    "            measurement = measurement.reshape(1, -1)\n",
    "\n",
    "        ###############\n",
    "        ### Free to change whatever you want below as long as hidden layers have size self.hidden_size\n",
    "        ### and output layer has size self.output_size\n",
    "        ###############\n",
    "\n",
    "        gru_cell = nn.GRUCell(\n",
    "            features=self.hidden_size,\n",
    "            gate_fn=nn.sigmoid,\n",
    "            activation_fn=nn.tanh,\n",
    "        )\n",
    "        self.make_rng('dropout')\n",
    "\n",
    "        new_hidden_state, _ = gru_cell(hidden_state, measurement)\n",
    "        new_hidden_state = nn.Dropout(rate=0.1, deterministic=False)(\n",
    "            new_hidden_state\n",
    "        )\n",
    "        # this returns the povm_params after linear regression through the hidden state which contains\n",
    "        # the information of the previous time steps and this is optimized to output best povm_params\n",
    "        # new_hidden_state = nn.Dense(features=self.hidden_size)(new_hidden_state)\n",
    "        new_hidden_state = nn.Dense(\n",
    "            features=self.hidden_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "        )(new_hidden_state)\n",
    "        new_hidden_state = nn.relu(new_hidden_state)\n",
    "        new_hidden_state = nn.Dense(\n",
    "            features=self.hidden_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "        )(new_hidden_state)\n",
    "        new_hidden_state = nn.relu(new_hidden_state)\n",
    "        output = nn.Dense(\n",
    "            features=self.output_size,\n",
    "            kernel_init=nn.initializers.glorot_uniform(),\n",
    "            bias_init=nn.initializers.constant(0.1),\n",
    "        )(new_hidden_state)\n",
    "        output = nn.relu(output)\n",
    "\n",
    "        ###############\n",
    "        ### Do not change the return statement\n",
    "        ###############\n",
    "\n",
    "        return output[0], new_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846472f",
   "metadata": {},
   "source": [
    "### In this notebook, we decreased the convergence threshold and evaluate for num_time_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f91cd24",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     61\u001b[39m system_params = [decay, measure, decay, displacement, snap, displacement_dag]\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m reward_weights \u001b[38;5;129;01min\u001b[39;00m [[\u001b[32m1.0\u001b[39m,\u001b[32m1.0\u001b[39m],[\u001b[32m0.0\u001b[39m,\u001b[32m1.0\u001b[39m]]:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     result = \u001b[43moptimize_pulse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mU_0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrho_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mC_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrho_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_time_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreward_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgoal\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfidelity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdensity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRNN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrnn_hidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_time_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreward weights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward_weights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m fidelity@t=2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.fidelity_each_timestep[\u001b[32m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m fidelity_each_timestep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjnp.mean(jnp.array(result.fidelity_each_timestep),\u001b[38;5;250m \u001b[39maxis=\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/zeropoint/u/adbo/notebooks/feedbackgrape/fgrape-on-ising-1D/feedback-grape/docs/source/tutorials/feedbackGRAPE-tutorials/../../../../feedback_grape/fgrape.py:730\u001b[39m, in \u001b[36moptimize_pulse\u001b[39m\u001b[34m(U_0, C_target, system_params, num_time_steps, max_iter, convergence_threshold, learning_rate, evo_type, reward_weights, goal, batch_size, eval_batch_size, eval_time_steps, mode, rnn, rnn_hidden_size, progress)\u001b[39m\n\u001b[32m    726\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_sum1 + loss_sum2\n\u001b[32m    728\u001b[39m train_key, eval_key = jax.random.split(train_eval_key)\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m best_model_params, iter_idx = \u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m result = _evaluate(\n\u001b[32m    742\u001b[39m     U_0=U_0,\n\u001b[32m    743\u001b[39m     C_target=C_target,\n\u001b[32m   (...)\u001b[39m\u001b[32m    759\u001b[39m     num_iterations=iter_idx,\n\u001b[32m    760\u001b[39m )\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/zeropoint/u/adbo/notebooks/feedbackgrape/fgrape-on-ising-1D/feedback-grape/docs/source/tutorials/feedbackGRAPE-tutorials/../../../../feedback_grape/fgrape.py:780\u001b[39m, in \u001b[36m_train\u001b[39m\u001b[34m(loss_fn, trainable_params, prng_key, max_iter, learning_rate, convergence_threshold, progress, early_stop)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[33;03mTrain the model using the specified optimizer.\u001b[39;00m\n\u001b[32m    777\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    778\u001b[39m \u001b[38;5;66;03m# Optimization\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[38;5;66;03m# set up optimizer and training state\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m best_model_params, iter_idx = \u001b[43moptimize_adam_feedback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvergence_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;66;03m# Due to the complex parameter l-bfgs is very slow and leads to bad results so is omitted\u001b[39;00m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_model_params, iter_idx\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/zeropoint/u/adbo/notebooks/feedbackgrape/fgrape-on-ising-1D/feedback-grape/docs/source/tutorials/feedbackGRAPE-tutorials/../../../../feedback_grape/utils/optimizers.py:54\u001b[39m, in \u001b[36moptimize_adam_feedback\u001b[39m\u001b[34m(loss_fn, control_amplitudes, max_iter, learning_rate, convergence_threshold, key, progress, early_stop)\u001b[39m\n\u001b[32m     52\u001b[39m iter_idx = -\u001b[32m1\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iter_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     new_params, new_opt_state, loss, key = \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     losses.append(loss)\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m early_stop:\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda-envs/newenv/lib/python3.13/site-packages/jax/_src/pjit.py:292\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    288\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    289\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    291\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, box_data,\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m  executable, pgle_profiler) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    295\u001b[39m     executable, out_tree, args_flat, out_flat, attrs_tracked, box_data,\n\u001b[32m    296\u001b[39m     jaxpr.effects, jaxpr.consts, jit_info.abstracted_axes, pgle_profiler)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda-envs/newenv/lib/python3.13/site-packages/jax/_src/pjit.py:153\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m   args_flat = \u001b[38;5;28mmap\u001b[39m(core.full_lower, args_flat)\n\u001b[32m    152\u001b[39m   core.check_eval_args(args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   out_flat, compiled, profiler = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    155\u001b[39m   out_flat = pjit_p.bind(*args_flat, **p.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda-envs/newenv/lib/python3.13/site-packages/jax/_src/pjit.py:1877\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1869\u001b[39m     fingerprint = fingerprint.hex()\n\u001b[32m   1870\u001b[39m   distributed_debug_log((\u001b[33m\"\u001b[39m\u001b[33mRunning pjit\u001b[39m\u001b[33m'\u001b[39m\u001b[33md function\u001b[39m\u001b[33m\"\u001b[39m, name),\n\u001b[32m   1871\u001b[39m                         (\u001b[33m\"\u001b[39m\u001b[33min_shardings\u001b[39m\u001b[33m\"\u001b[39m, in_shardings),\n\u001b[32m   1872\u001b[39m                         (\u001b[33m\"\u001b[39m\u001b[33mout_shardings\u001b[39m\u001b[33m\"\u001b[39m, out_shardings),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1875\u001b[39m                         (\u001b[33m\"\u001b[39m\u001b[33mabstract args\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mmap\u001b[39m(core.abstractify, args)),\n\u001b[32m   1876\u001b[39m                         (\u001b[33m\"\u001b[39m\u001b[33mfingerprint\u001b[39m\u001b[33m\"\u001b[39m, fingerprint))\n\u001b[32m-> \u001b[39m\u001b[32m1877\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsafe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m, compiled, pgle_profiler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda-envs/newenv/lib/python3.13/site-packages/jax/_src/profiler.py:354\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    353\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda-envs/newenv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:1297\u001b[39m, in \u001b[36mExecuteReplicated.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.ordered_effects \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_unordered_effects\n\u001b[32m   1295\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_host_callbacks):\n\u001b[32m   1296\u001b[39m   input_bufs = \u001b[38;5;28mself\u001b[39m._add_tokens_to_inputs(input_bufs)\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m   results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxla_executable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m      \u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m   1299\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1301\u001b[39m   result_token_bufs = results.disassemble_prefix_into_single_device_arrays(\n\u001b[32m   1302\u001b[39m       \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.ordered_effects))\n\u001b[32m   1303\u001b[39m   sharded_runtime_token = results.consume_token()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Note if tsave = jnp.linspace(0, 1, 1) = [0.0] then the decay is not applied ?\n",
    "# because the first time step has the original non decayed state\n",
    "from feedback_grape.fgrape import Decay, Gate\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Answer: In documentation, clarify that the initial_params are the params up to the\n",
    "# point where measurement occurs, compared with other modes where the initial_params\n",
    "# are the initial params for the entire system for all time steps. --> this is already fixed in\n",
    "# example a and an explanation of the mechanism may be provided in the docs\n",
    "measure = Gate(\n",
    "    gate=povm_measure_operator,\n",
    "    initial_params=jax.random.uniform(\n",
    "        key,\n",
    "        shape=(2,),  # 2 for gamma and delta\n",
    "        minval=-jnp.pi / 2,\n",
    "        maxval=jnp.pi / 2,\n",
    "        dtype=jnp.float64,\n",
    "    ),\n",
    "    measurement_flag=True,\n",
    ")\n",
    "\n",
    "displacement = Gate(\n",
    "    gate=displacement_gate,\n",
    "    initial_params=jax.random.uniform(\n",
    "        key,\n",
    "        shape=(2,),\n",
    "        minval=-jnp.pi / 2,\n",
    "        maxval=jnp.pi / 2,\n",
    "        dtype=jnp.float64,\n",
    "    ),\n",
    "    measurement_flag=False,\n",
    ")\n",
    "\n",
    "snap = Gate(\n",
    "    gate=snap_gate,\n",
    "    initial_params=jax.random.uniform(\n",
    "        key,\n",
    "        shape=(N_snap,),\n",
    "        minval=-jnp.pi / 2,\n",
    "        maxval=jnp.pi / 2,\n",
    "        dtype=jnp.float64,\n",
    "    ),\n",
    "    measurement_flag=False,\n",
    ")\n",
    "\n",
    "displacement_dag = Gate(\n",
    "    gate=displacement_gate_dag,\n",
    "    initial_params=jax.random.uniform(\n",
    "        key,\n",
    "        shape=(2,),\n",
    "        minval=-jnp.pi / 2,\n",
    "        maxval=jnp.pi / 2,\n",
    "        dtype=jnp.float64,\n",
    "    ),\n",
    "    measurement_flag=False,\n",
    ")\n",
    "\n",
    "decay = Decay(c_ops=[jnp.sqrt(0.005) * destroy(N_cav)])\n",
    "\n",
    "system_params = [decay, measure, decay, displacement, snap, displacement_dag]\n",
    "\n",
    "for reward_weights in [[1.0,1.0],[0.0,1.0]]:\n",
    "    result = optimize_pulse(\n",
    "        U_0=rho_target,\n",
    "        C_target=rho_target,\n",
    "        system_params=system_params,\n",
    "        num_time_steps=2,\n",
    "        reward_weights=reward_weights,\n",
    "        mode=\"nn\",\n",
    "        goal=\"fidelity\",\n",
    "        max_iter=1000,\n",
    "        convergence_threshold=1e-6,\n",
    "        learning_rate=0.01,\n",
    "        evo_type=\"density\",\n",
    "        batch_size=16,\n",
    "        rnn=RNN,\n",
    "        rnn_hidden_size=30,\n",
    "        eval_batch_size=16,\n",
    "        eval_time_steps=5,\n",
    "    )\n",
    "\n",
    "    print(f\"reward weights: {reward_weights}\\n fidelity@t=2: {result.fidelity_each_timestep[2]}\\n fidelity_each_timestep: {jnp.mean(jnp.array(result.fidelity_each_timestep), axis=1)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee6481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([1.00000003, 1.00000003, 1.00000003, 1.00000003, 1.00000003,\n",
       "        1.00000003, 1.00000003, 1.00000003, 1.00000003, 1.00000003,\n",
       "        1.00000003, 1.00000003, 1.00000003, 1.00000003, 1.00000003,\n",
       "        1.00000003], dtype=float64),\n",
       " Array([0.96154967, 0.96154967, 0.96154967, 0.96154967, 0.96154967,\n",
       "        0.96154967, 0.96154967, 0.95779168, 0.96154967, 0.96154967,\n",
       "        0.96154967, 0.96154967, 0.96154967, 0.96154967, 0.96154967,\n",
       "        0.96154967], dtype=float64),\n",
       " Array([0.92614202, 0.92614204, 0.92614204, 0.92614202, 0.92614203,\n",
       "        0.92614204, 0.92614195, 0.92363866, 0.926142  , 0.92614203,\n",
       "        0.92614201, 0.92614191, 0.92614204, 0.92614203, 0.92614202,\n",
       "        0.92614189], dtype=float64),\n",
       " Array([0.89358022, 0.89358059, 0.89358117, 0.89358102, 0.8935811 ,\n",
       "        0.89358075, 0.89357949, 0.89204237, 0.89357968, 0.89358122,\n",
       "        0.89358   , 0.89357994, 0.89358113, 0.89358018, 0.89358094,\n",
       "        0.89357986], dtype=float64),\n",
       " Array([0.86359781, 0.86359847, 0.86360226, 0.86360207, 0.86360165,\n",
       "        0.86360008, 0.86359681, 0.86277921, 0.86359738, 0.86359999,\n",
       "        0.86359809, 0.8635978 , 0.86360215, 0.86359794, 0.86359945,\n",
       "        0.86359834], dtype=float64),\n",
       " Array([0.83596127, 0.83596185, 0.83596754, 0.83596642, 0.83596562,\n",
       "        0.83596414, 0.83596061, 0.83564785, 0.83596105, 0.83596339,\n",
       "        0.83596186, 0.83596143, 0.83596625, 0.83596158, 0.83596343,\n",
       "        0.83596212], dtype=float64)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.fidelity_each_timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24452f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 30, 30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.final_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8321286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedback_grape.utils.fidelity import ket2dm\n",
    "\n",
    "N_cav = 30  # number of cavity modes\n",
    "N_snap = 15\n",
    "\n",
    "alpha = 2\n",
    "psi_target = coherent(N_cav, alpha) + coherent(N_cav, -alpha)\n",
    "\n",
    "# Normalize psi_target before constructing rho_target\n",
    "psi_target = psi_target / jnp.linalg.norm(psi_target)\n",
    "\n",
    "rho_target = ket2dm(psi_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ec5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial fidelity: 1.000000032429954\n",
      "fidelity of state 0: 0.8359612711998172\n",
      "fidelity of state 1: 0.8359618467417742\n",
      "fidelity of state 2: 0.8359675355739266\n",
      "fidelity of state 3: 0.835966415029211\n",
      "fidelity of state 4: 0.8359656164946603\n",
      "fidelity of state 5: 0.8359641410816578\n",
      "fidelity of state 6: 0.8359606103227433\n",
      "fidelity of state 7: 0.8356478528631098\n",
      "fidelity of state 8: 0.8359610534699193\n",
      "fidelity of state 9: 0.8359633933801333\n",
      "fidelity of state 10: 0.8359618560276743\n",
      "fidelity of state 11: 0.8359614345557127\n",
      "fidelity of state 12: 0.8359662548975771\n",
      "fidelity of state 13: 0.8359615784064238\n",
      "fidelity of state 14: 0.8359634338000455\n",
      "fidelity of state 15: 0.8359621154355017\n"
     ]
    }
   ],
   "source": [
    "from feedback_grape.utils.fidelity import fidelity\n",
    "\n",
    "print(\n",
    "    \"initial fidelity:\",\n",
    "    fidelity(C_target=rho_target, U_final=rho_target, evo_type=\"density\"),\n",
    ")\n",
    "for i, state in enumerate(result.final_state):\n",
    "    print(\n",
    "        f\"fidelity of state {i}:\",\n",
    "        fidelity(C_target=rho_target, U_final=state, evo_type=\"density\"),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fgrape_paper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
