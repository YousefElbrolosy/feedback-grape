{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f0c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "from flax import linen as nn  # Linen API\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "# We create one dense layer(model) instance (taking 'features' parameter as input)\n",
    "model = nn.Dense(features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f142fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': (5,), 'kernel': (10, 5)}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1, key2 = random.split(random.key(0))\n",
    "x = random.normal(key1, (10,))  # Dummy input data\n",
    "params = model.init(key2, x)  # Initialization call\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params)  # Checking output shapes\n",
    "# 10 rows (training examples) and 5 columns (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f0330",
   "metadata": {},
   "source": [
    "To conduct a forward pass with the model with a given set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b6881e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.471482  , -0.16962777,  0.12994334,  0.5224348 , -1.0919424 ],      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98fe2d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Set problem dimensions.\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.key(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "# Store the parameters in a FrozenDict pytree.\n",
    "true_params = flax.core.freeze({'params': {'bias': b, 'kernel': W}})\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = (\n",
    "    jnp.dot(x_samples, W)\n",
    "    + b\n",
    "    + 0.1 * random.normal(key_noise, (n_samples, y_dim))\n",
    ")\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24cdaf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as JAX version but using model.apply().\n",
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "    # Define the squared loss for a single pair (x,y)\n",
    "    def squared_error(x, y):\n",
    "        pred = model.apply(params, x)\n",
    "        return jnp.inner(y - pred, y - pred) / 2.0\n",
    "\n",
    "    # Vectorize the previous to compute the average of the loss on all samples.\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a57718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.023887426\n",
      "Loss step 0:  29.167133\n",
      "Loss step 10:  0.6585776\n",
      "Loss step 20:  0.2847663\n",
      "Loss step 30:  0.15943417\n",
      "Loss step 40:  0.097042\n",
      "Loss step 50:  0.06356495\n",
      "Loss step 60:  0.04459983\n",
      "Loss step 70:  0.03334882\n",
      "Loss step 80:  0.026425159\n",
      "Loss step 90:  0.022047115\n",
      "Loss step 100:  0.019225018\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "    params = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params, grads\n",
    "    )\n",
    "    return params\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    # Perform one gradient update.\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    params = update_params(params, learning_rate, grads)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Loss step {i}: ', loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91b5ce",
   "metadata": {},
   "source": [
    "### Using Optax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd603165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d57620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  0.019003797\n",
      "Loss step 10:  0.20683758\n",
      "Loss step 20:  0.05855435\n",
      "Loss step 30:  0.035229336\n",
      "Loss step 40:  0.02006433\n",
      "Loss step 50:  0.016343607\n",
      "Loss step 60:  0.014753598\n",
      "Loss step 70:  0.014125167\n",
      "Loss step 80:  0.013897793\n",
      "Loss step 90:  0.013803817\n",
      "Loss step 100:  0.013771151\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 10 == 0:\n",
    "        print('Loss step {}: '.format(i), loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96983d0a",
   "metadata": {},
   "source": [
    "If you want to save your optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fd98ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict output\n",
      "{'params': {'bias': Array([-2.4378996 , -2.07345   ,  0.18866651, -0.34174547, -0.77294886],      dtype=float32), 'kernel': Array([[ 0.99648714, -0.84069055, -0.7420563 , -1.1751673 , -0.88212633],\n",
      "       [ 0.5849409 ,  0.6981158 , -1.0097519 ,  1.6940992 , -1.9293189 ],\n",
      "       [-1.2534724 ,  0.19594063, -1.1317803 ,  0.23212159,  1.7384187 ],\n",
      "       [ 0.54630667,  0.62694055, -0.4886673 ,  0.9249355 ,  0.3472473 ],\n",
      "       [-0.6366119 , -0.96825767,  0.77167565,  1.0612171 ,  0.9169658 ],\n",
      "       [-0.46407035,  0.8665426 ,  1.6763489 , -2.526709  ,  0.4433026 ],\n",
      "       [ 1.8060067 , -1.2587421 , -0.57777286,  2.2216108 ,  0.76148087],\n",
      "       [-0.21333523, -1.6236352 , -0.8071298 , -2.3925495 ,  1.5795202 ],\n",
      "       [ 1.4050099 ,  0.34845546,  0.02730806,  1.1249974 , -0.21001515],\n",
      "       [-0.30623946,  0.7411661 , -0.3360593 ,  0.46161816,  0.19004533]],      dtype=float32)}}\n",
      "Bytes output\n",
      "b'\\x81\\xa6params\\x82\\xa4bias\\xc7!\\x01\\x93\\x91\\x05\\xa7float32\\xc4\\x14\\x8c\\x06\\x1c\\xc0h\\xb3\\x04\\xc0\\xcb1A>C\\xf9\\xae\\xbe\\xfa\\xdfE\\xbf\\xa6kernel\\xc7\\xd6\\x01\\x93\\x92\\n\\x05\\xa7float32\\xc4\\xc8\\xc8\\x19\\x7f?\\x7f7W\\xbfg\\xf7=\\xbf\\xe2k\\x96\\xbf\\x08\\xd3a\\xbf\\xb0\\xbe\\x15?\\xb8\\xb72?\\x8d?\\x81\\xbf>\\xd8\\xd8?\\xec\\xf3\\xf6\\xbf\\xc9q\\xa0\\xbf\\xa9\\xa4H>-\\xde\\x90\\xbfH\\xb1m>\\x81\\x84\\xde?\\xc1\\xda\\x0b?-\\x7f ?\\x9a2\\xfa\\xbe\\x93\\xc8l?f\\xca\\xb1>\\xff\\xf8\"\\xbf\\xbc\\xdfw\\xbf\\x89\\x8cE?\\xf6\\xd5\\x87?E\\xbej?\\xa1\\x9a\\xed\\xbe\\xbc\\xd5]?\\x9a\\x92\\xd6?\\x9a\\xb5!\\xc0\\x8f\\xf8\\xe2>:+\\xe7?v\\x1e\\xa1\\xbf\\xec\\xe8\\x13\\xbf\\xdf.\\x0e@i\\xf0B?\\x8dtZ\\xbeG\\xd3\\xcf\\xbf\\x0f\\xa0N\\xbf\\x88\\x1f\\x19\\xc0\\xb8-\\xca?]\\xd7\\xb3?\\xc1h\\xb2>(\\xb5\\xdf<\\xea\\xff\\x8f?6\\x0eW\\xbek\\xcb\\x9c\\xbe\\x10\\xbd=?\\xf7\\x0f\\xac\\xbe7Y\\xec>>\\x9bB>'\n"
     ]
    }
   ],
   "source": [
    "from flax import serialization\n",
    "\n",
    "bytes_output = serialization.to_bytes(params)\n",
    "dict_output = serialization.to_state_dict(params)\n",
    "print('Dict output')\n",
    "print(dict_output)\n",
    "print('Bytes output')\n",
    "print(bytes_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac6e01",
   "metadata": {},
   "source": [
    "Retrieving optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ad37728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': array([-2.4378996 , -2.07345   ,  0.18866651, -0.34174547, -0.77294886],\n",
       "        dtype=float32),\n",
       "  'kernel': array([[ 0.99648714, -0.84069055, -0.7420563 , -1.1751673 , -0.88212633],\n",
       "         [ 0.5849409 ,  0.6981158 , -1.0097519 ,  1.6940992 , -1.9293189 ],\n",
       "         [-1.2534724 ,  0.19594063, -1.1317803 ,  0.23212159,  1.7384187 ],\n",
       "         [ 0.54630667,  0.62694055, -0.4886673 ,  0.9249355 ,  0.3472473 ],\n",
       "         [-0.6366119 , -0.96825767,  0.77167565,  1.0612171 ,  0.9169658 ],\n",
       "         [-0.46407035,  0.8665426 ,  1.6763489 , -2.526709  ,  0.4433026 ],\n",
       "         [ 1.8060067 , -1.2587421 , -0.57777286,  2.2216108 ,  0.76148087],\n",
       "         [-0.21333523, -1.6236352 , -0.8071298 , -2.3925495 ,  1.5795202 ],\n",
       "         [ 1.4050099 ,  0.34845546,  0.02730806,  1.1249974 , -0.21001515],\n",
       "         [-0.30623946,  0.7411661 , -0.3360593 ,  0.46161816,  0.19004533]],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialization.from_bytes(params, bytes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "057a6b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax  # for optimizers\n",
    "from flax.training import train_state\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aa4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # x: [batch_size, time_steps, input_size]\n",
    "        batch_size, time_steps, _ = x.shape\n",
    "\n",
    "        # Specify the number of features for the GRU cell\n",
    "        gru_cell = nn.GRUCell(features=self.hidden_size)\n",
    "\n",
    "        # Initial hidden state\n",
    "        h = jnp.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            h, _ = gru_cell(h, x[:, t])  # Apply GRU cell at each time step\n",
    "            outputs.append(h)\n",
    "\n",
    "        # Stack outputs: [time_steps, batch_size, hidden_size] → [batch_size, time_steps, hidden_size]\n",
    "        outputs = jnp.stack(outputs, axis=1)\n",
    "\n",
    "        # Final projection (e.g., for classification or regression)\n",
    "        logits = nn.Dense(self.output_size)(outputs)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6269735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fb235ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    one_hot = jax.nn.one_hot(labels, logits.shape[-1])\n",
    "    return optax.softmax_cross_entropy(logits, one_hot).mean()\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    x, y = batch\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = GRUModel(hidden_size=64, output_size=10).apply(\n",
    "            {'params': params}, x\n",
    "        )\n",
    "        loss = cross_entropy_loss(logits[:, -1], y)  # last time step\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22ba3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data\n",
    "x = np.random.randn(32, 10, 16).astype(\n",
    "    jnp.float32\n",
    ")  # [batch, time, input_size]\n",
    "y = np.random.randint(0, 10, (32,))\n",
    "\n",
    "model = GRUModel(hidden_size=64, output_size=10)\n",
    "params = model.init(jax.random.PRNGKey(0), x)['params']\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply, params=params, tx=optax.adam(1e-3)\n",
    ")\n",
    "\n",
    "# Train step\n",
    "state = train_step(state, (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c33c6a",
   "metadata": {},
   "source": [
    "# Part 2 Creating Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66b8488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'layers_0': {'bias': (16,), 'kernel': (4, 16)}, 'layers_1': {'bias': (8,), 'kernel': (16, 8)}, 'layers_2': {'bias': (1,), 'kernel': (8, 1)}}}\n",
      "Output: [[-0.12992406]\n",
      " [-0.02715186]\n",
      " [-0.10914332]\n",
      " [-0.13801154]]\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "class Brolos(nn.Module):\n",
    "    num_neurons_per_layer: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [nn.Dense(n) for n in self.num_neurons_per_layer]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        activation = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            activation = layer(activation)\n",
    "            if i < len(self.layers) - 1:\n",
    "                activation = nn.relu(activation)\n",
    "        return activation\n",
    "x_key, init_key = random.split(random.key(0))\n",
    "\n",
    "model = Brolos(num_neurons_per_layer=[16, 8, 1])\n",
    "x = random.uniform(x_key, (4, 4))\n",
    "params = model.init(init_key, x)\n",
    "y = model.apply(params, x)\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, params))\n",
    "print(f\"Output: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fc0eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'layers_0': {'bias': (16,), 'kernel': (4, 16)}, 'layers_1': {'bias': (8,), 'kernel': (16, 8)}, 'layers_2': {'bias': (1,), 'kernel': (8, 1)}}}\n",
      "Output: [[-0.12992406]\n",
      " [-0.02715186]\n",
      " [-0.10914332]\n",
      " [-0.13801154]]\n"
     ]
    }
   ],
   "source": [
    "# instead of using setup, we can use the @nn.compact decorator\n",
    "class BrolosCompact(nn.Module):\n",
    "    num_neurons_per_layer: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        activation = x\n",
    "        for i, n in enumerate(self.num_neurons_per_layer):\n",
    "            activation = nn.Dense(n)(activation)\n",
    "            if i < len(self.num_neurons_per_layer) - 1:\n",
    "                activation = nn.relu()\n",
    "        return activation\n",
    "    \n",
    "model = Brolos(num_neurons_per_layer=[16, 8, 1])\n",
    "x = random.uniform(x_key, (4, 4))\n",
    "params = model.init(init_key, x)\n",
    "y = model.apply(params, x)\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, params))\n",
    "print(f\"Output: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad756278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'bias': (3,), 'weight': (4, 3)}}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'pprint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(params, x)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mshape, params))\n\u001b[0;32m---> 24\u001b[0m \u001b[43mpprint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpprint\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'pprint'"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "from pprint import pprint\n",
    "\n",
    "class MyDenseImp(nn.Module):\n",
    "    num_neurons: int\n",
    "    weight_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # name that appears in the dict\n",
    "        weight = self.param('weight', self.weight_init, # initialization function\n",
    "                             (x.shape[-1], self.num_neurons))\n",
    "        bias = self.param('bias', self.bias_init, (self.num_neurons,))\n",
    "        return jnp.dot(x, weight) + bias\n",
    "    \n",
    "x_key, init_key = random.split(random.key(0))\n",
    "\n",
    "model = MyDenseImp(num_neurons=3)\n",
    "x = random.uniform(x_key, (4, 4))\n",
    "params = model.init(init_key, x)\n",
    "y = model.apply(params, x)\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, params))\n",
    "pprint(f\"Output: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b7536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit-stable8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
